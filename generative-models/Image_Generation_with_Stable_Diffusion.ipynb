{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPDVZzJT5aAM3ofdp7EbQPs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahamsi/computer-vision/blob/main/generative-models/Image_Generation_with_Stable_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/tahamsi/computer-vision)"
      ],
      "metadata": {
        "id": "6B_jDnDwb54N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Generation with Sable Diffusion\n",
        "\n",
        "[Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion) is a deep learning model developed by Stability AI that generates detailed images based on textual descriptions. Released in 2022, it utilizes diffusion techniques to create high-quality, photorealistic images from text prompts.\n",
        "\n",
        "##Key Features\n",
        "\n",
        "* Text-to-Image Generation: Transforms textual prompts into corresponding images, enabling users to visualize concepts described in words.\n",
        "\n",
        "* Open-Source Availability: Unlike some proprietary models, Stable Diffusion's code and model weights are publicly accessible, allowing for community-driven development and customization.\n",
        "\n",
        "* Versatility: Beyond generating images from text, it supports tasks like inpainting (filling in missing parts of an image), outpainting (extending images beyond their original borders), and image-to-image translations guided by text prompts.\n",
        "\n",
        "## Diffusion Models\n",
        "Diffusion models, such as DDPM and DDIM, generate data by adding noise step-by-step during a forward process and then learning to reverse this noise in a backward process to recover the original data. They are particularly effective in generating realistic images, but they can be slow since thousands of denoising steps are often required."
      ],
      "metadata": {
        "id": "GZcLVvy8bMJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Diffusion Model (LDM):\n",
        "\n",
        "The core of Stable Diffusion is a Latent Diffusion Model, where diffusion happens in a latent space rather than directly on pixel data. This is computationally efficient while still generating high-quality outputs.\n",
        "The diffusion process iteratively denoises latent representations to reach a desired output.\n"
      ],
      "metadata": {
        "id": "O5fR9KZbdRZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two Main Phases\n",
        "* A. Forward Diffusion Process (Adding Noise)\n",
        "Similar to DDPM, noise is progressively added to the input during the forward process, resulting in an image that eventually becomes indistinguishable from pure noise after enough steps.\n",
        "* B. Reverse Denoising Process (Generating Images)\n",
        "During inference, the model starts with random noise and applies a sequence of denoising steps to reconstruct a high-quality image.\n",
        "In Stable Diffusion, this denoising is done in the latent space, using a neural network known as the U-Net."
      ],
      "metadata": {
        "id": "tPRIL4QMdwJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m-qLJzKFd8O_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##UNet Architecture\n",
        "\n",
        "The denoising component of the model is based on a U-Net architecture. This is a type of neural network particularly well-suited for capturing multi-scale information. The U-Net is responsible for reversing the noise process to generate a coherent image.\n"
      ],
      "metadata": {
        "id": "_jJJ4Oibf0TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Encoder\n",
        "A key part of Stable Diffusion is its use of a text encoder, such as [CLIP](https://github.com/tahamsi/computer-vision/blob/main/week-9/CLIP.ipynb)â€™s text model, to encode user-provided text prompts into a latent space. These text embeddings are then used as conditioning inputs, which guide the image generation process based on the prompt.\n",
        "\n"
      ],
      "metadata": {
        "id": "fI2H6-RsgDE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scheduler\n",
        "\n",
        "Stable Diffusion uses a scheduler to guide the denoising process, typically employing a predefined number of steps. Popular schedulers like PNDM (Predictive Noise Diffusion Models) or DDIM (Denoising Diffusion Implicit Models) are used to balance between image quality and speed.\n"
      ],
      "metadata": {
        "id": "OLzWDSiMgLS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Autoencoder (VAE)\n",
        "\n",
        "The model uses an Autoencoder (VAE - Variational Autoencoder) to convert images to latent representations and back. It helps to reduce the dimensionality of images, which makes the diffusion process more efficient."
      ],
      "metadata": {
        "id": "bzcL3ejygOiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classifier-Free Guidance:\n",
        "\n",
        "This guidance method is used during image generation to improve the output quality. It combines conditional and unconditional denoising results to guide the generated images more closely towards the desired prompt, giving better control over the strength of conditioning.\n"
      ],
      "metadata": {
        "id": "e4Vf8tRGgUJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##How it Works Together\n",
        "The process starts by encoding a prompt through the text encoder.\n",
        "Latent noise is initialized, which the U-Net model iteratively denoises based on the text conditioning.\n",
        "A scheduler controls the pace of denoising.\n",
        "The output of the U-Net is passed through the VAE to convert the latent representation into a high-resolution image."
      ],
      "metadata": {
        "id": "mUaEzbCeeVcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to `GPU`. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit -> Notebook settings -> Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "fvMaErlUcAAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "exosmjr4byGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "1eCUN_2icHRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision diffusers transformers\n",
        "from IPython import display\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "kMWZKEr9cO6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required Libraries"
      ],
      "metadata": {
        "id": "n082wehVeuSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import matplotlib.pyplot as plt\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "QsAPxbxue0J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Initialize the model"
      ],
      "metadata": {
        "id": "ViIbtFfEgy1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJVKH0_MbKX6"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained Stable Diffusion model\n",
        "# model_id = \"CompVis/stable-diffusion-v1-4\" # SDv 1\n",
        "model_id = \"stabilityai/stable-diffusion-2-base\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Load the Stable Diffusion v2 model pipeline\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)  # Move the pipeline to GPU for faster generation\n",
        "\n",
        "# Generate an image based on a text prompt\n",
        "def generate_image(prompt, output_path=\"generated_image.png\"):\n",
        "    with torch.autocast(device):\n",
        "        image = pipe(prompt).images[0]\n",
        "\n",
        "    # Save the generated image\n",
        "    image.save(output_path)\n",
        "    print(f\"Image saved to {output_path}\")\n",
        "    return image\n",
        "display.clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test\n",
        "\n",
        "Test the model by passing a prompt."
      ],
      "metadata": {
        "id": "GkZMgF1DkM2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "output_path=\"generated_image.png\"\n",
        "prompt = \"A scenic view of a futuristic cityscape during sunset\"\n",
        "image = generate_image(prompt)\n",
        "\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "hEAoRnTSiXMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions in the package\n",
        "* Core Functions: from_pretrained(), __call__()\n",
        "* Memory Optimization: enable_attention_slicing(), enable_xformers_memory_efficient_attention(), enable_sequential_cpu_offload()\n",
        "* Device Management: to()\n",
        "* Saving and Loading: save_pretrained()\n",
        "* Safety Management: disable_safety_checker()\n",
        "* UI Controls: set_progress_bar_config()\n",
        "\n",
        "## Classes within StableDiffusionPipeline\n",
        "\n",
        "* UNet2DConditionModel: The denoising model that takes the noisy latent representation and tries to denoise it progressively, guided by the conditioning (the prompt).\n",
        "* AutoencoderKL (VAE): Used for encoding images into the latent space and decoding latent representations back into images. It helps reduce the computational complexity.\n",
        "* CLIPTextModel: This text encoder encodes the prompt into embeddings, which condition the denoising process.\n",
        "* Scheduler (e.g., PNDMScheduler, DDIMScheduler): Guides the denoising process. Different schedulers can be used to balance speed and quality of the generated image.\n",
        "* SafetyChecker: (Optional) Ensures that generated images are safe and appropriate.\n",
        "* FeatureExtractor: Sometimes used to preprocess inputs or validate outputs (in cases where extra processing is needed)."
      ],
      "metadata": {
        "id": "ZuYcbP3Zk4Lo"
      }
    }
  ]
}