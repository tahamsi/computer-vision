{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5os9oN0JNHhlu6Ce6w3oa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahamsi/computer-vision/blob/main/generative-models/DDPM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/tahamsi/computer-vision)"
      ],
      "metadata": {
        "id": "ipOE56E2dqaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Denoising Diffusion Probabilistic Models"
      ],
      "metadata": {
        "id": "gPSrOprKvtT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Overview\n",
        "Denoising Diffusion Probabilistic Models ([DDPMs](https://arxiv.org/abs/2006.11239)) are a class of generative models introduced by Jonathan Ho, Ajay Jain, and Pieter Abbeel in 2020. These models generate data by reversing a diffusion process that incrementally adds noise to the data. By learning to reverse this noising process, DDPMs can produce high-quality samples from complex data distributions."
      ],
      "metadata": {
        "id": "ADn63Dlpd_NM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process can be described in two main phases: the **forward** process (**diffusion**) and the **reverse** process (**generation**).\n",
        "\n",
        "### Forward Diffusion Process (adding noise)\n",
        "The forward process is where noise is gradually added to the data over a series of time steps. It starts with an image $𝑥_0$ (e.g., a picture of a cat). And gradually adds Gaussian noise to it over $𝑇$ steps using this formula:\n",
        "\n",
        "$$q(\\mathbf{x}_t \\mid \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\, \\mathbf{x}_0, (1 - \\bar{\\alpha}_t) \\mathbf{I}).$$\n",
        "\n",
        "Or\n",
        "\n",
        "$$ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon.$$\n",
        "\n",
        "where:\n",
        "* $\\bar{\\alpha}_t = \\prod_{s=1}^{t} (1 - \\beta_t)$\n",
        "* $\\beta_t$: A small variance schedule controlling how much noise is added at step $𝑡$\n",
        "* $𝑥_𝑡$: Noisy image at step $𝑡$,\n",
        "\n",
        "* $\\epsilon$: Random (Gaussian) noise (like static on a TV).\n",
        "\n",
        "By the end ($𝑡$ = $𝑇$), the image looks like pure noise.\n",
        "\n",
        "### Reverse Process (Denoising)\n",
        "The model (a neural network) learns to reverse this noise-adding process step-by-step to recover the original image or generate a new one. The reverse process is defined as:\n",
        "\n",
        "$$p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_\\theta(\\mathbf{x}_t, t), \\Sigma_\\theta(\\mathbf{x}_t, t)).$$\n",
        "\n",
        "Where:\n",
        "* $\\mu_\\theta(\\mathbf{x}_t, t)$: The predicted mean, parameterized by a neural network.\n",
        "* $\\Sigma_\\theta(\\mathbf{x}_t, t)$: The variance, often fixed for simplicity.\n",
        "\n",
        "In another word, at each step, the model predicts the noise $\\epsilon_θ(x_t,t)$ using a neural network and removes it:\n",
        "\n",
        "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\sqrt{1 - \\alpha_t} \\, \\epsilon_\\theta(x_t, t) \\right).$$\n",
        "\n",
        "where:\n",
        "* $x_{t-1}$: The less noisy image at step $𝑡-1$,\n",
        "* $\\epsilon_\\theta(x_t, t)$: Noise predicted by the model.\n",
        "\n",
        "The process is repeated until a clean image is generated.\n"
      ],
      "metadata": {
        "id": "5wgNgCoQfu9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "To train the model, we calculate how well it predicts the noise $ϵ$. The simplified loss function is:\n",
        "$$\\mathcal{L}_t = \\mathbb{E}_q \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right].$$\n",
        "\n",
        "This measures the difference between:\n",
        "\n",
        "* $\\epsilon$: The actual noise added,\n",
        "* $\\epsilon_\\theta(x_t, t)$: The noise predicted by the model.\n",
        "\n",
        "### Full Objective\n",
        "The total loss over all $𝑇$ timesteps is:\n",
        "\n",
        "$$\\mathcal{L} = \\sum_{t=1}^T \\mathcal{L}_t = \\mathbb{E}_q \\left[ \\sum_{t=1}^T \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right]$$\n"
      ],
      "metadata": {
        "id": "yBA5FH-Wp2zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training\n",
        "\n",
        "\\\\(\\mathbf{x}_0\\\\) represents the initial real and uncorrupted image, while \\\\(t\\\\) denotes the noise level sampled through the fixed forward process. \\\\(\\mathbf{\\epsilon}\\\\) corresponds to the true Gaussian noise sampled at time step \\\\(t\\\\), and \\\\(\\mathbf{\\epsilon}_\\theta (\\mathbf{x}_t, t)\\\\) is the output of the neural network. The network is trained to minimize the mean squared error (MSE) between the true Gaussian noise \\\\(\\mathbf{\\epsilon}\\\\) and its predicted counterpart \\\\(\\mathbf{\\epsilon}_\\theta (\\mathbf{x}_t, t)\\\\).\n",
        "\n",
        "The training algorithm now looks as follows:\n",
        "\n",
        "> **Algorithm 1**\n",
        "> 1. **repeat**\n",
        "> 2. $x_0 \\sim q(x_0)$\n",
        "> 3. $t \\sim Uniform({1,...,T})$\n",
        "> 4. $ ϵ ∼ \\mathcal{N}(0,\\mathbf{I})$\n",
        "> 5. Take gradient descent step on\n",
        ">   * $\\nabla_θ\\| \\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t) \\|^2$\n",
        "> 6. **until** converged\n",
        "\n",
        "\n",
        "In other words:\n",
        "\n",
        "* A random sample \\\\(\\mathbf{x}_0\\\\) is taken from the real, unknown, and potentially complex data distribution \\\\(q(\\mathbf{x}_0)\\\\).\n",
        "* A noise level \\\\(t\\\\) is sampled uniformly from the range \\\\([1, T]\\\\), representing a random time step.\n",
        "* Noise is sampled from a Gaussian distribution, and the input \\\\(\\mathbf{x}_0\\\\) is corrupted at level \\\\(t\\\\) using the property defined earlier.\n",
        "* The neural network is trained to predict the noise based on the corrupted image \\\\(\\mathbf{x}_t\\\\), which is \\\\(\\mathbf{x}_0\\\\) corrupted according to a predefined schedule \\\\(\\beta_t\\\\)."
      ],
      "metadata": {
        "id": "yNdzg_h4wzYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://raw.githubusercontent.com/tahamsi/computer-vision/refs/heads/main/images/ddpm.jpeg\"/>\n",
        "</p>"
      ],
      "metadata": {
        "id": "G9L_R1PsxNxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The neural network\n",
        "A noised image at a specific time step is provided to the neural network, and the predicted noise is returned as output. The predicted noise is represented as a tensor with the same size and resolution as the input image. Thus, the network is required to process and output tensors of identical shapes. The question arises: what type of neural network is suitable for this task?\n",
        "\n",
        "Autoencoders are characterized by a \"*bottleneck*\" layer located between the encoder and decoder. The encoder compresses an image into a smaller hidden representation called the \"bottleneck,\" while the decoder reconstructs the hidden representation back into an image. This architecture compels the network to retain only the most essential information in the bottleneck layer.\n",
        "\n",
        "For this purpose, the DDPM authors selected a **U-Net** architecture, originally introduced by [Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597), which at the time demonstrated state-of-the-art performance in medical image segmentation. Like autoencoders, the U-Net architecture includes a bottleneck in the middle to ensure that only critical information is learned by the network. Additionally, U-Net introduced residual connections between the encoder and decoder, significantly enhancing gradient flow.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/unet-model.png\" width=\"500\" />\n",
        "</p>\n",
        "\n",
        "source: [huggingface](https://huggingface.co)"
      ],
      "metadata": {
        "id": "XY-MZkKQw9qF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling\n",
        "\n",
        "The model will be sampled during training (to track progress), the corresponding code is defined below based on Algorithm 2 in the [paper](https://arxiv.org/abs/2006.11239).\n",
        "\n",
        "\n",
        "> **Algorithm 2**\n",
        "> * $x_T \\sim \\mathcal{N}(0,\\mathbf{I})$\n",
        "> * `for` $t$ `in` ${T,...,1}$:\n",
        ">   * `if` $t>1$:\n",
        ">     * $ ϵ ∼ \\mathcal{N}(0,\\mathbf{I})$\n",
        ">   * `else`:\n",
        ">     * $ϵ = 0$\n",
        ">   * $x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-α_t}{\\sqrt{1 - \\alpha_t}} \\, \\epsilon_\\theta(x_t, t) \\right) + σ_t ϵ$\n",
        "> * `return` $x_0$\n",
        "\n",
        "New images are generated from a diffusion model by reversing the diffusion process. The process begins at time step \\\\(T\\\\), where pure noise is sampled from a Gaussian distribution. The noise is then gradually denoised using the neural network, which leverages the conditional probabilities it has learned, until time step \\\\(t = 0\\\\) is reached. As described earlier, a slightly less denoised image \\\\(\\mathbf{x}_{t-1}\\\\) can be derived by substituting the reparameterized mean into the noise predictor. It should be noted that the variance is predetermined. Ideally, an image is produced that appears as though it originated from the real data distribution.\n"
      ],
      "metadata": {
        "id": "grl30gdMFEgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Why It Works\n",
        "* The model learns to reverse the noisy process, step by step.\n",
        "* By minimizing $\\mathcal{L}$, it becomes better at reconstructing $x_0$ or generating new, realistic data from noise.\n"
      ],
      "metadata": {
        "id": "VCSA1E2BweRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Ideas in Simple Terms\n",
        "* Forward Process: Gradually make the image noisier.\n",
        "* Reverse Process: Teach the model to remove the noise step-by-step.\n",
        "* Final Output: A clear image generated from random noise.\n"
      ],
      "metadata": {
        "id": "CG6lxOFvnhix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why It’s Powerful\n",
        "* Produces high-quality images.\n",
        "* Stable to train compared to other methods like GANs.\n",
        "* Can handle tasks like:\n",
        " * Image generation (e.g., create art from scratch).\n",
        " *Denoising (e.g., clean up blurry or noisy images).\n",
        " *Inpainting (e.g., fill in missing parts of an image)."
      ],
      "metadata": {
        "id": "dxjUQ3nqop72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This simplified process is the foundation of tools like [Stable Diffusion](https://github.com/tahamsi/computer-vision/blob/main/generative-models/Image_Generation_with_Stable_Diffusion.ipynb) for generating images from text!"
      ],
      "metadata": {
        "id": "Ehb9jtMjo9Xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Implimentation in pytorch"
      ],
      "metadata": {
        "id": "yHFq2D8KEj_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before you start\n",
        "\n",
        "Let's make sure that we have access to `GPU`. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit -> Notebook settings -> Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "B3Ibg_NTdxlh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP1w_lVAdlLX"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required libraries"
      ],
      "metadata": {
        "id": "5eJjQoEZ_4uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade einops datasets matplotlib tqdm\n",
        "from IPython import display\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "PzF5iDEX_9PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "lTOHagGHHpjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from einops import rearrange\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "AQA8JdC77Rgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network helpers\n",
        "\n",
        "First, some helper functions and classes are defined, which will be utilized in the implementation of the neural network. Notably, a `Residual` module is defined, which adds the input to the output of a specific function (i.e., a residual connection is added to the function)."
      ],
      "metadata": {
        "id": "cIX44mCF7KO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "def Upsample(dim):\n",
        "    return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "def Downsample(dim):\n",
        "    return nn.Conv2d(dim, dim, 4, 2, 1)"
      ],
      "metadata": {
        "id": "khtj5f9E7a2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position embeddings\n",
        "Since the parameters of the neural network are shared across time (noise levels), sinusoidal position embeddings are employed to encode \\(t\\), following the approach inspired by the Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)). This allows the neural network to \"understand\" the specific time step (noise level) at which it is operating for each image in a batch.\n",
        "\n",
        "The `SinusoidalPositionEmbeddings` module takes a tensor of shape \\\\((\\text{batch_size}, 1)\\\\) as input, representing the noise levels of multiple noisy images in a batch, and transforms it into a tensor of shape \\\\((\\text{batch_size}, \\text{dim})\\\\), where \\\\(\\text{dim}\\\\) is the dimensionality of the position embeddings. These embeddings are then added to each residual block, as will be demonstrated later."
      ],
      "metadata": {
        "id": "9lUaL_Ja7b9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "b2CBwHgv7fJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet/ConvNeXT block\n",
        "\n",
        "Next, the core building block of the U-Net model is defined."
      ],
      "metadata": {
        "id": "SIGeiwp07iXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, groups = 8):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
        "        self.norm = nn.GroupNorm(groups, dim_out)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, scale_shift = None):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.block1 = Block(dim, dim_out, groups=groups)\n",
        "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        h = self.block1(x)\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            h = rearrange(time_emb, \"b c -> b c 1 1\") + h\n",
        "\n",
        "        h = self.block2(h)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class ConvNextBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, mult=2, norm=True):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.GELU(), nn.Linear(time_emb_dim, dim))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding=3, groups=dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.GroupNorm(1, dim) if norm else nn.Identity(),\n",
        "            nn.Conv2d(dim, dim_out * mult, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.GroupNorm(1, dim_out * mult),\n",
        "            nn.Conv2d(dim_out * mult, dim_out, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        h = self.ds_conv(x)\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            assert exists(time_emb), \"time embedding must be passed in\"\n",
        "            condition = self.mlp(time_emb)\n",
        "            h = h + rearrange(condition, \"b c -> b c 1 1\")\n",
        "\n",
        "        h = self.net(h)\n",
        "        return h + self.res_conv(x)"
      ],
      "metadata": {
        "id": "1t_l2CwP7k_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention module\n",
        "\n",
        "Next, the attention module, which was added by the DDPM authors between the convolutional blocks, is defined. Attention is the building block of the famous Transformer architecture ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), which has shown great success in various domains of AI, from NLP and vision to [protein folding](https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)."
      ],
      "metadata": {
        "id": "qmX8HTl57p02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "        q = q * self.scale\n",
        "\n",
        "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
        "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
        "                                    nn.GroupNorm(1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "        return self.to_out(out)"
      ],
      "metadata": {
        "id": "OWFVW5wU7tBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Group normalization\n",
        "\n",
        "The convolutional and attention layers of the U-Net are interleaved with group normalization by the DDPM authors. A `PreNorm` class is defined below, which is used to apply group normalization before the attention layer, as will be demonstrated later."
      ],
      "metadata": {
        "id": "2httFGtg7uJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.GroupNorm(1, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)"
      ],
      "metadata": {
        "id": "J3hW87KK7wjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conditional U-Net\n",
        "\n",
        "Now that all the building blocks have been defined (position embeddings, ResNet/ConvNeXT blocks, attention, and group normalization), the entire neural network can be constructed. The purpose of the network \\\\(\\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\\\) is to process a batch of noisy images and corresponding noise levels and output the noise added to the input. Formally:\n",
        "\n",
        "A batch of noisy images with shape `(batch_size, num_channels, height, width)` and a batch of noise levels with shape `(batch_size, 1)` are provided as input, and a tensor of shape `(batch_size, num_channels, height, width)` is returned as output.\n",
        "\n",
        "The network is structured as follows:\n",
        "\n",
        "* First, a convolutional layer is applied to the batch of noisy images, and position embeddings are computed for the noise levels.\n",
        "* Next, a sequence of downsampling stages is applied. Each downsampling stage consists of two ResNet/ConvNeXT blocks, group normalization, attention, residual connections, and a downsampling operation.\n",
        "* At the middle of the network, ResNet or ConvNeXT blocks are applied, interleaved with attention layers.\n",
        "* Subsequently, a sequence of upsampling stages is applied. Each upsampling stage consists of two ResNet/ConvNeXT blocks, group normalization, attention, residual connections, and an upsampling operation.\n",
        "* Finally, a ResNet/ConvNeXT block is applied, followed by a convolutional layer."
      ],
      "metadata": {
        "id": "teosKIpU7zYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        init_dim=None,\n",
        "        out_dim=None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        channels=3,\n",
        "        with_time_emb=True,\n",
        "        resnet_block_groups=8,\n",
        "        use_convnext=True,\n",
        "        convnext_mult=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # determine dimensions\n",
        "        self.channels = channels\n",
        "\n",
        "        init_dim = default(init_dim, dim // 3 * 2)\n",
        "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
        "\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        if use_convnext:\n",
        "            block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
        "        else:\n",
        "            block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
        "\n",
        "        # time embeddings\n",
        "        if with_time_emb:\n",
        "            time_dim = dim * 4\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEmbeddings(dim),\n",
        "                nn.Linear(dim, time_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(time_dim, time_dim),\n",
        "            )\n",
        "        else:\n",
        "            time_dim = None\n",
        "            self.time_mlp = None\n",
        "\n",
        "        # layers\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                        Downsample(dim_out) if not is_last else nn.Identity(),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.ups.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                        Upsample(dim_in) if not is_last else nn.Identity(),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        out_dim = default(out_dim, channels)\n",
        "        self.final_conv = nn.Sequential(\n",
        "            block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
        "\n",
        "        h = []\n",
        "\n",
        "        # downsample\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        # bottleneck\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        # upsample\n",
        "        for block1, block2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            x = upsample(x)\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "6rQGdKvf731f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the forward diffusion process\n",
        "\n",
        "The forward diffusion process is designed to gradually add noise to an image from the real distribution over a series of time steps \\\\(T\\\\). This process is governed by a variance schedule. A linear schedule was employed by the original DDPM authors, as follows:\n",
        "\n",
        ">The forward process variances are set to constants that increase linearly from \\\\(\\beta_1 = 10^{-4}\\\\) to \\\\(\\beta_T = 0.02\\\\).\n",
        "\n",
        "Various schedules for the \\\\(T\\\\) time steps are defined below, along with corresponding variables, such as cumulative variances, which will be required."
      ],
      "metadata": {
        "id": "SQBFRjG576v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "def linear_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "def quadratic_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(-6, 6, timesteps)\n",
        "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start"
      ],
      "metadata": {
        "id": "_0OPn28i77tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin, the linear schedule is used for \\\\(T=200\\\\) time steps, and the various variables derived from \\\\(\\beta_t\\\\), such as the cumulative product of the variances \\\\(\\bar{\\alpha}_t\\\\), are defined. Each of these variables is represented as a one-dimensional tensor, storing values from \\\\(t\\\\) to \\\\(T\\\\). Additionally, an extract function is defined to enable the extraction of the appropriate \\\\(t\\\\) index for a given batch of indices."
      ],
      "metadata": {
        "id": "ETlieVRK7-7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = 200\n",
        "\n",
        "# define beta schedule\n",
        "betas = linear_beta_schedule(timesteps=timesteps)\n",
        "\n",
        "# define alphas\n",
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "\n",
        "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "\n",
        "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    batch_size = t.shape[0]\n",
        "    out = a.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
      ],
      "metadata": {
        "id": "U_Nov5NK8CKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/tahamsi/computer-vision/refs/heads/main/images/London_bridge.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image"
      ],
      "metadata": {
        "id": "AeYWUM4r8GGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise is added to PyTorch tensors rather than Pillow images. To facilitate this, image transformations are defined to convert a PIL image into a PyTorch tensor (on which noise can be added) and back to a PIL image.\n",
        "\n",
        "These transformations are relatively straightforward: images are first normalized by dividing by \\\\(255\\\\) to bring them into the \\\\([0, 1]\\\\) range and then scaled to the \\\\([-1, 1]\\\\) range. From the DDPM paper:\n",
        "\n",
        "> Image data is assumed to consist of integers in \\\\({0, 1, ..., 255}\\\\) scaled linearly to \\\\([-1, 1]\\\\). This ensures that the neural network reverse process operates on consistently scaled inputs, starting from the standard normal prior \\\\(p(\\mathbf{x}_T)\\\\)."
      ],
      "metadata": {
        "id": "Kcl9ZroS8I7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
        "\n",
        "image_size = 128\n",
        "transform = Compose([\n",
        "    Resize(image_size),\n",
        "    CenterCrop(image_size),\n",
        "    ToTensor(), # turn into Numpy array of shape HWC, divide by 255\n",
        "    Lambda(lambda t: (t * 2) - 1),\n",
        "\n",
        "])\n",
        "\n",
        "x_start = transform(image).unsqueeze(0)\n",
        "x_start.shape"
      ],
      "metadata": {
        "id": "x11Yd6Um8LP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "reverse_transform = Compose([\n",
        "     Lambda(lambda t: (t + 1) / 2),\n",
        "     Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "     Lambda(lambda t: t * 255.),\n",
        "     Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "     ToPILImage(),\n",
        "])"
      ],
      "metadata": {
        "id": "vkqsnVxX8Ndy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_transform(x_start.squeeze())"
      ],
      "metadata": {
        "id": "3FJWTxJE8QGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward diffusion\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
        "    )\n",
        "\n",
        "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise"
      ],
      "metadata": {
        "id": "OF-xWMUb8SSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_noisy_image(x_start, t):\n",
        "  # add noise\n",
        "  x_noisy = q_sample(x_start, t=t)\n",
        "\n",
        "  # turn back into PIL image\n",
        "  noisy_image = reverse_transform(x_noisy.squeeze())\n",
        "\n",
        "  return noisy_image"
      ],
      "metadata": {
        "id": "9fvfHjTH8Uf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take time step\n",
        "t = torch.tensor([40])\n",
        "\n",
        "get_noisy_image(x_start, t)"
      ],
      "metadata": {
        "id": "3mAK5cSG8WKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Visualisation"
      ],
      "metadata": {
        "id": "IAc3dVac8ZV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# use seed for reproducability\n",
        "torch.manual_seed(0)\n",
        "\n",
        "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0]) + with_orig\n",
        "    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        row = [image] + row if with_orig else row\n",
        "        for col_idx, img in enumerate(row):\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    if with_orig:\n",
        "        axs[0, 0].set(title='Original image')\n",
        "        axs[0, 0].title.set_size(8)\n",
        "    if row_title is not None:\n",
        "        for row_idx in range(num_rows):\n",
        "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "mKOx4Iks8bMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])"
      ],
      "metadata": {
        "id": "UlZbvjDV8dsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
        "    predicted_noise = denoise_model(x_noisy, t)\n",
        "\n",
        "    if loss_type == 'l1':\n",
        "        loss = F.l1_loss(noise, predicted_noise)\n",
        "    elif loss_type == 'l2':\n",
        "        loss = F.mse_loss(noise, predicted_noise)\n",
        "    elif loss_type == \"huber\":\n",
        "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "f5njz1YE8gGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `denoise_model` will be our U-Net defined above. We'll employ the Huber loss between the true and the predicted noise.\n",
        "\n",
        "### Training\n",
        "\n",
        "Each image is resized to a uniform size. Notably, images are also randomly flipped horizontally. As stated in the paper:\n",
        "\n",
        "> Random horizontal flips were used during training for `CIFAR10`. Training was conducted both with and without flips, and it was observed that flips slightly improved sample quality."
      ],
      "metadata": {
        "id": "aoV5ZlVO8jGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load dataset from the hub\n",
        "dataset = load_dataset(\"fashion_mnist\")\n",
        "image_size = 28\n",
        "channels = 1\n",
        "batch_size = 128\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "ajXSNQSb8mUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, a function is defined to be applied on-the-fly to the entire dataset. The `with_transform` [functionality](https://huggingface.co/docs/datasets/v2.2.1/en/package_reference/main_classes#datasets.Dataset.with_transform) is utilized for this purpose. This function performs basic image preprocessing, including random horizontal flips, rescaling, and scaling the values to the \\([-1, 1]\\) range."
      ],
      "metadata": {
        "id": "E0-4MWlC8ony"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# define image transformations (e.g. using torchvision)\n",
        "transform = Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "])\n",
        "\n",
        "# define function\n",
        "def transforms(examples):\n",
        "   examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
        "   del examples[\"image\"]\n",
        "\n",
        "   return examples\n",
        "\n",
        "transformed_dataset = dataset.with_transform(transforms).remove_columns(\"label\")\n",
        "\n",
        "# create dataloader\n",
        "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ZzOmAsek8rBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(dataloader))\n",
        "print(batch.keys())"
      ],
      "metadata": {
        "id": "6R5aTbId8tgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling\n",
        "\n",
        "The code below implements this."
      ],
      "metadata": {
        "id": "VD-jdlq38wBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, t_index):\n",
        "    betas_t = extract(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "    )\n",
        "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
        "\n",
        "    # Equation 11 in the paper\n",
        "    # Use our model (noise predictor) to predict the mean\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "\n",
        "    if t_index == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "        noise = torch.randn_like(x)\n",
        "        # Algorithm 2 line 4:\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "# Algorithm 2 but save all images:\n",
        "@torch.no_grad()\n",
        "def p_sample_loop(model, shape):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    b = shape[0]\n",
        "    # start from pure noise (for each example in the batch)\n",
        "    img = torch.randn(shape, device=device)\n",
        "    imgs = []\n",
        "\n",
        "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
        "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
        "        imgs.append(img.cpu().numpy())\n",
        "    return imgs\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, image_size, batch_size=16, channels=3):\n",
        "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))"
      ],
      "metadata": {
        "id": "aipZ4Hxi8ytF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "results_folder = Path(\"./results\")\n",
        "results_folder.mkdir(exist_ok = True)\n",
        "save_and_sample_every = 1000"
      ],
      "metadata": {
        "id": "cRqWPL5D84J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = Unet(\n",
        "    dim=image_size,\n",
        "    channels=channels,\n",
        "    dim_mults=(1, 2, 4,)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "XoycgIcA88B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_size = batch[\"pixel_values\"].shape[0]\n",
        "      batch = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "      # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
        "      t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
        "\n",
        "      loss = p_losses(model, batch, t, loss_type=\"huber\")\n",
        "\n",
        "      if step % 100 == 0:\n",
        "        print(\"Loss:\", loss.item())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # save generated images\n",
        "      if step != 0 and step % save_and_sample_every == 0:\n",
        "        milestone = step // save_and_sample_every\n",
        "        batches = num_to_groups(4, batch_size)\n",
        "        all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))\n",
        "        all_images = torch.cat(all_images_list, dim=0)\n",
        "        all_images = (all_images + 1) * 0.5\n",
        "        save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)"
      ],
      "metadata": {
        "id": "EwHVvdLz9B9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "m0TlbWYo9E5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "samples = sample(model, image_size=image_size, batch_size=batch_size, channels=channels)"
      ],
      "metadata": {
        "id": "L4Dd9onS9G79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show a random one\n",
        "random_index = np.random.choice(batch_size)\n",
        "plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap=\"gray\")"
      ],
      "metadata": {
        "id": "d8STxiaM9Irj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.animation as animation\n",
        "from IPython.display import Image\n",
        "\n",
        "random_index = np.random.choice(batch_size)\n",
        "\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for i in range(timesteps):\n",
        "    im = plt.imshow(samples[i][random_index].reshape(image_size, image_size, channels), cmap=\"gray\", animated=True)\n",
        "    ims.append([im])\n",
        "\n",
        "animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
        "animate.save('diffusion.gif')\n",
        "plt.show()\n",
        "Image(open('diffusion.gif','rb').read())"
      ],
      "metadata": {
        "id": "38j8QB7g9NJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Huggingface Diffusers"
      ],
      "metadata": {
        "id": "-Vi3Gdk0RVQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview"
      ],
      "metadata": {
        "id": "qlcubG2QeiRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without delving too deeply into the details, the model is typically not trained to directly predict a slightly less noisy image. Instead, it is trained to predict the \"noise residual,\" which represents the difference between a less noisy image and the input image (as in the diffusion model called \"DDPM\"), or alternatively, the gradient between two time steps (as in the diffusion model called \"Score VE\").\n",
        "\n",
        "To perform the denoising process, a specific noise scheduling algorithm is required. This algorithm is used to \"wrap\" the model, defining the number of diffusion steps needed for inference and determining how to compute a less noisy image from the model's output. This is where the various schedulers from the diffusers library are utilized.\n",
        "\n",
        "Lastly, a **pipeline** combines a **model** and a **scheduler**, simplifying the process for end-users to execute a complete denoising loop. The focus will begin with pipelines, exploring their implementation, before examining models and schedulers in greater detail."
      ],
      "metadata": {
        "id": "mXk-7zgafDw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `diffusers`"
      ],
      "metadata": {
        "id": "2NvPpOgA9ZZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers\n",
        "from IPython import display\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "RnoG8jN19caM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipelines"
      ],
      "metadata": {
        "id": "hy-3KuZ995sU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process is initiated with the import of a pipeline. The google/ddpm-celebahq-256 model, which was collaboratively developed by Google and U.C. Berkeley, is utilized. This model follows the DDPM algorithm and has been trained on a dataset of celebrity images.\n",
        "\n",
        "The `DDPMPipeline` is imported to enable inference with minimal code. The `from_pretrained()` method is then used to download the model and its configuration from [the Hugging Face Hub](https://huggingface.co/google/ddpm-celebahq-256), a repository hosting over 60,000 community-contributed models."
      ],
      "metadata": {
        "id": "tNvfnzZ099e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DDPMPipeline\n",
        "image_pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\n",
        "image_pipe.to(\"cuda\")\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "OoYl1W-b-EoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate an image, the pipeline is run without requiring any input. A random initial noise sample is automatically generated, and the diffusion process is iterated. A dictionary containing the generated `sample` of interest is returned as output. On Google Colab, this process typically requires 2–3 minutes."
      ],
      "metadata": {
        "id": "oPmV0CQv-O9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = image_pipe().images\n",
        "images[0]"
      ],
      "metadata": {
        "id": "fY56R1XV-S2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s explore the components that make up the pipeline:"
      ],
      "metadata": {
        "id": "nFtiPxi7-dwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_pipe"
      ],
      "metadata": {
        "id": "lFVR5f1U-mEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inside the pipeline, a scheduler and a U-Net model can be observed. A closer examination of these components and the operations performed by the pipeline behind the scenes is provided."
      ],
      "metadata": {
        "id": "Bcvd57dv-qdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models\n",
        "\n",
        "Instances of the model class are defined as neural networks that take a noisy `sample` and a `timestep` as inputs to predict a less noisy output sample. A pre-trained model is loaded and explored to gain an understanding of the model's API.\n",
        "\n",
        "An unconditional image generation model of type `UNet2DModel` is loaded, and another checkpoint trained on church images, [`google/ddpm-ema-cat-256`](https://huggingface.co/google/ddpm-ema-cat-256), is examined.\n",
        "\n",
        "Similar to the pipeline class, the model configuration and weights can be loaded with a single line using the `from_pretrained()` method, which may already be familiar from the `transformers` library."
      ],
      "metadata": {
        "id": "QSmNi_Ph-rWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DModel\n",
        "repo_id = \"google/ddpm-ema-cat-256\"\n",
        "model = UNet2DModel.from_pretrained(repo_id)\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "bu-_fb8z-jQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s examine the model’s configuration. The configuration is accessible through the `config` attribute and displays all the essential parameters required to define the model architecture (and nothing more)."
      ],
      "metadata": {
        "id": "TXZf7A6w-33J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "XlUAL8j5-7_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The model configuration is presented as a frozen dictionary. This design ensures that the configuration is only used to define the model architecture during instantiation and is not modified for any attributes during inference.\n",
        "\n",
        "Several key configuration parameters are:\n",
        "\n",
        "* `sample_size`: Specifies the height and width dimensions of the input sample.\n",
        "* `in_channels`: Indicates the number of input channels in the input sample.\n",
        "* `down_block_types` and `up_block_types`: Define the types of downsampling and upsampling blocks used to construct the UNet architecture, as depicted earlier in the notebook.\n",
        "* `block_out_channels`: Specifies the number of output channels for the downsampling blocks, which are also used in reversed order for the input channels of the upsampling blocks.\n",
        "* `layers_per_block`: Defines the number of ResNet blocks included in each UNet block.\n",
        "\n",
        "With an understanding of the UNet configuration, the same model architecture can be instantiated with random weights. To achieve this, the configuration can be passed as an unpacked dictionary to the `UNet2DModel` class."
      ],
      "metadata": {
        "id": "9NNDL1kJ-_Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_random = UNet2DModel(**model.config)"
      ],
      "metadata": {
        "id": "fE1mtIB2_H_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statement above indicates that a randomly initialized model has been created with the same configuration as the previous one."
      ],
      "metadata": {
        "id": "6VLu92ur_JNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save the model that has just been created, the `save_pretrained()` method can be used. This method saves both the model's weights and its configuration in the specified folder.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fFmzyNYe_MlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_random.save_pretrained(\"my_model\")"
      ],
      "metadata": {
        "id": "IV0DbSi7_VYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls my_model"
      ],
      "metadata": {
        "id": "bkkAfaap_Zxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`diffusion_pytorch_model.bin` is a binary PyTorch file that stores the model weights and `config.json` stores the model's configuration."
      ],
      "metadata": {
        "id": "Fel4oU4W_dOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to reuse the model, you can simply use the `from_pretrained()` method again, as it loads local checkpoints as well as those present on the Hub."
      ],
      "metadata": {
        "id": "IymGF7Lt_pmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_random = UNet2DModel.from_pretrained(\"my_model\")"
      ],
      "metadata": {
        "id": "StV8EaK2_l5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Returning to the trained model, the process of using it for inference can now be explored. To begin, a random Gaussian sample is required with the shape of an image (`batch_size` \\\\(\\times\\\\) `in_channels` \\\\(\\times\\\\) `sample_size` \\\\(\\times\\\\) `sample_size`). The batch axis represents the model's ability to process multiple random noise samples simultaneously. The `channel` axis accounts for the multiple channels in each sample (e.g., red, green, blue for RGB images). Finally, the `sample_size` corresponds to the height and width of the image."
      ],
      "metadata": {
        "id": "iUNdvIRV_y2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "noisy_sample = torch.randn(\n",
        "    1, model.config.in_channels, model.config.sample_size, model.config.sample_size\n",
        ")\n",
        "noisy_sample.shape"
      ],
      "metadata": {
        "id": "rWPHO03W_1w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference\n",
        "\n",
        "The noisy sample can be passed through the model along with a `timestep`. The timestep is crucial as it informs the model about \"how noisy\" the input image is—more noisy at the beginning of the process and less noisy toward the end—enabling the model to determine its position in the diffusion process.\n",
        "\n",
        "As discussed in the introduction, the model predicts either the slightly less noisy image, the difference between the slightly less noisy image and the input image, or another related quantity. It is essential to review [`google/ddpm-ema-cat-256`](https://huggingface.co/google/ddpm-ema-cat-256) carefully to understand what the model has been trained to predict. In this specific case, the model predicts the noise residual, which is the difference between the slightly less noisy image and the input image."
      ],
      "metadata": {
        "id": "dHjR_6cS_2hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    noisy_residual = model(sample=noisy_sample, timestep=2).sample"
      ],
      "metadata": {
        "id": "TKoGxuP2_5_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predicted `noisy_residual` has the exact same shape as the input and we use it to compute a slightly less noised image. Let's confirm the output shapes match:"
      ],
      "metadata": {
        "id": "YEEuuDOS_-Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noisy_residual.shape"
      ],
      "metadata": {
        "id": "F53ZbZNzABPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "**models**, such as `UNet2DModel` are parameterized neural networks trained to *predict* a slightly less noisy image or residual. They are defined by their `.config` and can be loaded from the Hub as well as saved and loaded locally. The next step is learning how to combine this **model** with the correct **scheduler** to be able to actually generate images."
      ],
      "metadata": {
        "id": "AmQJf8hnAEJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Schedulers\n",
        "\n",
        "**Schedulers** are algorithms encapsulated within Python classes. They are used to define the noise schedule applied to the model during training and to compute the slightly less noisy sample based on the model output (in this case, the `noisy_residual`). This notebook focuses exclusively on the use of scheduler classes for inference. For details on using schedulers during training, another notebook can be referenced.\n",
        "\n",
        "It should be emphasized that, unlike models, which contain trainable weights, schedulers are typically parameter-free (meaning they do not possess trainable weights). Instead, they define the algorithm for computing the slightly less noisy sample. As a result, schedulers do not inherit from `torch.nn.Module` but, like models, are instantiated using a configuration. To download a scheduler configuration from the Hub, the `from_config()` method can be used to load the configuration and instantiate a scheduler."
      ],
      "metadata": {
        "id": "wFvIFYj-ALST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DDPMScheduler\n",
        "\n",
        "scheduler = DDPMScheduler.from_config(repo_id)\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "tgwkeuOyAHxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler.config"
      ],
      "metadata": {
        "id": "VN1-msftATlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the most important parameters:\n",
        "- `num_train_timesteps` defines the length of the denoising process, e.g. how many timesteps are need to process random gaussian noise to a data sample.\n",
        "- `beta_schedule` define the type of noise schedule that shall be used for inference and training\n",
        "- `beta_start` and `beta_end` define the smallest noise value and highest noise value of the schedule.\n",
        "\n",
        "Like the *models*, *schedulers* can be saved and loaded with `save_config()` and `from_config()`.\n"
      ],
      "metadata": {
        "id": "mahOUEK0AWUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler.save_config(\"my_scheduler\")\n",
        "new_scheduler = DDPMScheduler.from_config(\"my_scheduler\")"
      ],
      "metadata": {
        "id": "IFkITzSVAdY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All schedulers provide one or multiple `step()` methods that can be used to compute the slightly less noisy image. The `step()` method may vary from one scheduler to another, but normally expects at least the model output, the `timestep` and the current `noisy_sample`. Note that the `step()` method is somewhat of a black box function that \"just works\"."
      ],
      "metadata": {
        "id": "noNkw3nUAeG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "less_noisy_sample = scheduler.step(\n",
        "    model_output=noisy_residual, timestep=2, sample=noisy_sample\n",
        ").prev_sample\n",
        "less_noisy_sample.shape"
      ],
      "metadata": {
        "id": "rYOV8ozXAj4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "def display_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Image at step {i}\")\n",
        "    display(image_pil)"
      ],
      "metadata": {
        "id": "3m8W3t3cAqFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cuda\")\n",
        "noisy_sample = noisy_sample.to(\"cuda\")"
      ],
      "metadata": {
        "id": "LymeQ0jKAubC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is now time to define the denoising loop, which is straightforward for DDPM:\n",
        "\n",
        "1. The residual of the less noisy sample is predicted using the model.\n",
        "2. The less noisy sample is computed using the scheduler.\n",
        "\n",
        "Additionally, progress is displayed at every 50th step.\n",
        "\n",
        "It is important to note that the loop iterates over `scheduler.timesteps`, a tensor that defines the sequence of timesteps for the denoising process. Typically, the process proceeds in decreasing order of timesteps, starting from the total number of timesteps (e.g., 1000) and ending at 0. Depending on the GPU, this process may take up to a minute—enough time to reflect on what has been learned so far while watching a church being constructed from pure noise.\n"
      ],
      "metadata": {
        "id": "Gcb9qzjIAxpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "sample = noisy_sample\n",
        "\n",
        "for i, t in enumerate(tqdm.tqdm(scheduler.timesteps)):\n",
        "  # 1. predict noise residual\n",
        "  with torch.no_grad():\n",
        "      residual = model(sample, t).sample\n",
        "\n",
        "  # 2. compute less noisy image and set x_t -> x_t-1\n",
        "  sample = scheduler.step(residual, t, sample).prev_sample\n",
        "\n",
        "  # 3. optionally look at image\n",
        "  if (i + 1) % 100 == 0:\n",
        "      display_sample(sample, i + 1)"
      ],
      "metadata": {
        "id": "wFTdnoE8A3Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be observed that it takes a significant amount of time—approximately 800 steps—to begin seeing a somewhat meaningful shape. While the quality of the generated image is quite impressive, you might want to consider ways to speed up the image generation process."
      ],
      "metadata": {
        "id": "fmHQDaBxBCqB"
      }
    }
  ]
}