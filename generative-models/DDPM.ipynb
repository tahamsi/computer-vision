{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN++nrJN5LQ5GEBjFW6BOXj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahamsi/computer-vision/blob/main/generative-models/DDPM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/tahamsi/computer-vision)"
      ],
      "metadata": {
        "id": "ipOE56E2dqaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Denoising Diffusion Probabilistic Models"
      ],
      "metadata": {
        "id": "gPSrOprKvtT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Overview\n",
        "Denoising Diffusion Probabilistic Models ([DDPMs](https://arxiv.org/abs/2006.11239)) are a class of generative models introduced by Jonathan Ho, Ajay Jain, and Pieter Abbeel in 2020. These models generate data by reversing a diffusion process that incrementally adds noise to the data. By learning to reverse this noising process, DDPMs can produce high-quality samples from complex data distributions."
      ],
      "metadata": {
        "id": "ADn63Dlpd_NM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process can be described in two main phases: the **forward** process (**diffusion**) and the **reverse** process (**generation**).\n",
        "\n",
        "### Forward Diffusion Process (adding noise)\n",
        "The forward process is where noise is gradually added to the data over a series of time steps. It starts with an image $ð‘¥_0$ (e.g., a picture of a cat). And gradually adds Gaussian noise to it over $ð‘‡$ steps using this formula:\n",
        "\n",
        "$$q(\\mathbf{x}_t \\mid \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\, \\mathbf{x}_0, (1 - \\bar{\\alpha}_t) \\mathbf{I}).$$\n",
        "\n",
        "Or\n",
        "\n",
        "$$ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon.$$\n",
        "\n",
        "where:\n",
        "* $\\bar{\\alpha}_t = \\prod_{s=1}^{t} (1 - \\beta_t)$\n",
        "* $\\beta_t$: A small variance schedule controlling how much noise is added at step $ð‘¡$\n",
        "* $ð‘¥_ð‘¡$: Noisy image at step $ð‘¡$,\n",
        "\n",
        "* $\\epsilon$: Random (Gaussian) noise (like static on a TV).\n",
        "\n",
        "By the end ($ð‘¡$ = $ð‘‡$), the image looks like pure noise.\n",
        "\n",
        "### Reverse Process (Denoising)\n",
        "The model (a neural network) learns to reverse this noise-adding process step-by-step to recover the original image or generate a new one. The reverse process is defined as:\n",
        "\n",
        "$$p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_\\theta(\\mathbf{x}_t, t), \\Sigma_\\theta(\\mathbf{x}_t, t)).$$\n",
        "\n",
        "Where:\n",
        "* $\\mu_\\theta(\\mathbf{x}_t, t)$: The predicted mean, parameterized by a neural network.\n",
        "* $\\Sigma_\\theta(\\mathbf{x}_t, t)$: The variance, often fixed for simplicity.\n",
        "\n",
        "In another word, at each step, the model predicts the noise $\\epsilon_Î¸(x_t,t)$ using a neural network and removes it:\n",
        "\n",
        "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\sqrt{1 - \\alpha_t} \\, \\epsilon_\\theta(x_t, t) \\right).$$\n",
        "\n",
        "where:\n",
        "* $x_{t-1}$: The less noisy image at step $ð‘¡-1$,\n",
        "* $\\epsilon_\\theta(x_t, t)$: Noise predicted by the model.\n",
        "\n",
        "The process is repeated until a clean image is generated.\n"
      ],
      "metadata": {
        "id": "5wgNgCoQfu9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "To train the model, we calculate how well it predicts the noise $Ïµ$. The simplified loss function is:\n",
        "$$\\mathcal{L}_t = \\mathbb{E}_q \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right].$$\n",
        "\n",
        "This measures the difference between:\n",
        "\n",
        "* $\\epsilon$: The actual noise added,\n",
        "* $\\epsilon_\\theta(x_t, t)$: The noise predicted by the model.\n",
        "\n",
        "### Full Objective\n",
        "The total loss over all $ð‘‡$ timesteps is:\n",
        "\n",
        "$$\\mathcal{L} = \\sum_{t=1}^T \\mathcal{L}_t = \\mathbb{E}_q \\left[ \\sum_{t=1}^T \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right]$$\n"
      ],
      "metadata": {
        "id": "yBA5FH-Wp2zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training\n",
        "\n",
        "\\\\(\\mathbf{x}_0\\\\) represents the initial real and uncorrupted image, while \\\\(t\\\\) denotes the noise level sampled through the fixed forward process. \\\\(\\mathbf{\\epsilon}\\\\) corresponds to the true Gaussian noise sampled at time step \\\\(t\\\\), and \\\\(\\mathbf{\\epsilon}_\\theta (\\mathbf{x}_t, t)\\\\) is the output of the neural network. The network is trained to minimize the mean squared error (MSE) between the true Gaussian noise \\\\(\\mathbf{\\epsilon}\\\\) and its predicted counterpart \\\\(\\mathbf{\\epsilon}_\\theta (\\mathbf{x}_t, t)\\\\).\n",
        "\n",
        "The training algorithm now looks as follows:\n",
        "\n",
        "> **Algorithm 1**\n",
        "> 1. **repeat**\n",
        "> 2. $x_0 \\sim q(x_0)$\n",
        "> 3. $t \\sim Uniform({1,...,T})$\n",
        "> 4. $ Ïµ âˆ¼ \\mathcal{N}(0,\\mathbf{I})$\n",
        "> 5. Take gradient descent step on\n",
        ">   * $\\nabla_Î¸\\| \\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t) \\|^2$\n",
        "> 6. **until** converged\n",
        "\n",
        "\n",
        "In other words:\n",
        "\n",
        "* A random sample \\\\(\\mathbf{x}_0\\\\) is taken from the real, unknown, and potentially complex data distribution \\\\(q(\\mathbf{x}_0)\\\\).\n",
        "* A noise level \\\\(t\\\\) is sampled uniformly from the range \\\\([1, T]\\\\), representing a random time step.\n",
        "* Noise is sampled from a Gaussian distribution, and the input \\\\(\\mathbf{x}_0\\\\) is corrupted at level \\\\(t\\\\) using the property defined earlier.\n",
        "* The neural network is trained to predict the noise based on the corrupted image \\\\(\\mathbf{x}_t\\\\), which is \\\\(\\mathbf{x}_0\\\\) corrupted according to a predefined schedule \\\\(\\beta_t\\\\)."
      ],
      "metadata": {
        "id": "yNdzg_h4wzYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://raw.githubusercontent.com/tahamsi/computer-vision/refs/heads/main/images/ddpm.jpeg\"/>\n",
        "</p>"
      ],
      "metadata": {
        "id": "G9L_R1PsxNxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The neural network\n",
        "A noised image at a specific time step is provided to the neural network, and the predicted noise is returned as output. The predicted noise is represented as a tensor with the same size and resolution as the input image. Thus, the network is required to process and output tensors of identical shapes. The question arises: what type of neural network is suitable for this task?\n",
        "\n",
        "Autoencoders are characterized by a \"*bottleneck*\" layer located between the encoder and decoder. The encoder compresses an image into a smaller hidden representation called the \"bottleneck,\" while the decoder reconstructs the hidden representation back into an image. This architecture compels the network to retain only the most essential information in the bottleneck layer.\n",
        "\n",
        "For this purpose, the DDPM authors selected a **U-Net** architecture, originally introduced by [Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597), which at the time demonstrated state-of-the-art performance in medical image segmentation. Like autoencoders, the U-Net architecture includes a bottleneck in the middle to ensure that only critical information is learned by the network. Additionally, U-Net introduced residual connections between the encoder and decoder, significantly enhancing gradient flow.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/unet-model.png\" width=\"500\" />\n",
        "</p>\n",
        "\n",
        "source: [huggingface](https://huggingface.co)"
      ],
      "metadata": {
        "id": "XY-MZkKQw9qF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling\n",
        "\n",
        "The model will be sampled during training (to track progress), the corresponding code is defined below based on Algorithm 2 in the [paper](https://arxiv.org/abs/2006.11239).\n",
        "\n",
        "\n",
        "> **Algorithm 2**\n",
        "> 1. $x_T \\sim \\mathcal{N}(0,\\mathbf{I})$\n",
        "> 2. `for` $t$ `in` ${T,...,1}$:\n",
        ">  * `if` $t>1$:\n",
        ">     * $ Ïµ âˆ¼ \\mathcal{N}(0,\\mathbf{I})$\n",
        ">  * `else`:\n",
        ">     * $Ïµ = 0$\n",
        ">  * $x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-Î±_t}{\\sqrt{1 - \\alpha_t}} \\, \\epsilon_\\theta(x_t, t) \\right) + Ïƒ_t Ïµ$\n",
        "> 3. `return` $x_0$\n",
        "\n",
        "New images are generated from a diffusion model by reversing the diffusion process. The process begins at time step \\\\(T\\\\), where pure noise is sampled from a Gaussian distribution. The noise is then gradually denoised using the neural network, which leverages the conditional probabilities it has learned, until time step \\\\(t = 0\\\\) is reached. As described earlier, a slightly less denoised image \\\\(\\mathbf{x}_{t-1}\\\\) can be derived by substituting the reparameterized mean into the noise predictor. It should be noted that the variance is predetermined. Ideally, an image is produced that appears as though it originated from the real data distribution.\n"
      ],
      "metadata": {
        "id": "grl30gdMFEgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Why It Works\n",
        "* The model learns to reverse the noisy process, step by step.\n",
        "* By minimizing $\\mathcal{L}$, it becomes better at reconstructing $x_0$ or generating new, realistic data from noise.\n"
      ],
      "metadata": {
        "id": "VCSA1E2BweRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Ideas in Simple Terms\n",
        "* Forward Process: Gradually make the image noisier.\n",
        "* Reverse Process: Teach the model to remove the noise step-by-step.\n",
        "* Final Output: A clear image generated from random noise.\n"
      ],
      "metadata": {
        "id": "CG6lxOFvnhix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Itâ€™s Powerful\n",
        "* Produces high-quality images.\n",
        "* Stable to train compared to other methods like GANs.\n",
        "* Can handle tasks like:\n",
        " * Image generation (e.g., create art from scratch).\n",
        " *Denoising (e.g., clean up blurry or noisy images).\n",
        " *Inpainting (e.g., fill in missing parts of an image)."
      ],
      "metadata": {
        "id": "dxjUQ3nqop72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This simplified process is the foundation of tools like [Stable Diffusion](https://github.com/tahamsi/computer-vision/blob/main/generative-models/Image_Generation_with_Stable_Diffusion.ipynb) for generating images from text!"
      ],
      "metadata": {
        "id": "Ehb9jtMjo9Xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Implimentation in pytorch"
      ],
      "metadata": {
        "id": "yHFq2D8KEj_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before you start\n",
        "\n",
        "Let's make sure that we have access to `GPU`. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit -> Notebook settings -> Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "B3Ibg_NTdxlh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP1w_lVAdlLX"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U einops datasets matplotlib tqdm\n",
        "\n",
        "import math\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from einops import rearrange\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "AQA8JdC77Rgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network helpers\n",
        "\n",
        "First, we define some helper functions and classes which will be used when implementing the neural network. Importantly, we define a `Residual` module, which simply adds the input to the output of a particular function (in other words, adds a residual connection to a particular function).\n",
        "\n",
        "We also define aliases for the up- and downsampling operations."
      ],
      "metadata": {
        "id": "cIX44mCF7KO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "def Upsample(dim):\n",
        "    return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "def Downsample(dim):\n",
        "    return nn.Conv2d(dim, dim, 4, 2, 1)"
      ],
      "metadata": {
        "id": "khtj5f9E7a2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position embeddings\n",
        "\n",
        "As the parameters of the neural network are shared across time (noise level), the authors employ sinusoidal position embeddings to encode $t$, inspired by the Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)). This makes the neural network \"know\" at which particular time step (noise level) it is operating, for every image in a batch.\n",
        "\n",
        "The `SinusoidalPositionEmbeddings` module takes a tensor of shape `(batch_size, 1)` as input (i.e. the noise levels of several noisy images in a batch), and turns this into a tensor of shape `(batch_size, dim)`, with `dim` being the dimensionality of the position embeddings. This is then added to each residual block, as we will see further."
      ],
      "metadata": {
        "id": "9lUaL_Ja7b9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "b2CBwHgv7fJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet/ConvNeXT block\n",
        "\n",
        "Next, we define the core building block of the U-Net model. The DDPM authors employed a Wide ResNet block ([Zagoruyko et al., 2016](https://arxiv.org/abs/1605.07146)), but Phil Wang decided to also add support for a ConvNeXT block ([Liu et al., 2022](https://arxiv.org/abs/2201.03545)), as the latter has achieved great success in the image domain. One can choose one or another in the final U-Net architecture."
      ],
      "metadata": {
        "id": "SIGeiwp07iXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, groups = 8):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
        "        self.norm = nn.GroupNorm(groups, dim_out)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, scale_shift = None):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
        "\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.block1 = Block(dim, dim_out, groups=groups)\n",
        "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        h = self.block1(x)\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            h = rearrange(time_emb, \"b c -> b c 1 1\") + h\n",
        "\n",
        "        h = self.block2(h)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class ConvNextBlock(nn.Module):\n",
        "    \"\"\"https://arxiv.org/abs/2201.03545\"\"\"\n",
        "\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, mult=2, norm=True):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.GELU(), nn.Linear(time_emb_dim, dim))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding=3, groups=dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.GroupNorm(1, dim) if norm else nn.Identity(),\n",
        "            nn.Conv2d(dim, dim_out * mult, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.GroupNorm(1, dim_out * mult),\n",
        "            nn.Conv2d(dim_out * mult, dim_out, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        h = self.ds_conv(x)\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            assert exists(time_emb), \"time embedding must be passed in\"\n",
        "            condition = self.mlp(time_emb)\n",
        "            h = h + rearrange(condition, \"b c -> b c 1 1\")\n",
        "\n",
        "        h = self.net(h)\n",
        "        return h + self.res_conv(x)"
      ],
      "metadata": {
        "id": "1t_l2CwP7k_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention module\n",
        "\n",
        "Next, we define the attention module, which the DDPM authors added in between the convolutional blocks. Attention is the building block of the famous Transformer architecture ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), which has shown great success in various domains of AI, from NLP and vision to [protein folding](https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology). Phil Wang employs 2 variants of attention: one is regular multi-head self-attention (as used in the Transformer), the other one is a [linear attention variant](https://github.com/lucidrains/linear-attention-transformer) ([Shen et al., 2018](https://arxiv.org/abs/1812.01243)), whose time- and memory requirements scale linear in the sequence length, as opposed to quadratic for regular attention.\n",
        "\n",
        "For an extensive explanation of the attention mechanism, we refer the reader to Jay Allamar's [wonderful blog post](https://jalammar.github.io/illustrated-transformer/)."
      ],
      "metadata": {
        "id": "qmX8HTl57p02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "        q = q * self.scale\n",
        "\n",
        "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
        "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
        "                                    nn.GroupNorm(1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "        return self.to_out(out)"
      ],
      "metadata": {
        "id": "OWFVW5wU7tBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Group normalization\n",
        "\n",
        "The DDPM authors interleave the convolutional/attention layers of the U-Net with group normalization ([Wu et al., 2018](https://arxiv.org/abs/1803.08494)). Below, we define a `PreNorm` class, which will be used to apply groupnorm before the attention layer, as we'll see further. Note that there's been a [debate](https://tnq177.github.io/data/transformers_without_tears.pdf) about whether to apply normalization before or after attention in Transformers."
      ],
      "metadata": {
        "id": "2httFGtg7uJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.GroupNorm(1, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)"
      ],
      "metadata": {
        "id": "J3hW87KK7wjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conditional U-Net\n",
        "\n",
        "Now that we've defined all building blocks (position embeddings, ResNet/ConvNeXT blocks, attention and group normalization), it's time to define the entire neural network. Recall that the job of the network \\\\(\\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\\\) is to take in a batch of noisy images + noise levels, and output the noise added to the input. More formally:\n",
        "\n",
        "- the network takes a batch of noisy images of shape `(batch_size, num_channels, height, width)` and a batch of noise levels of shape `(batch_size, 1)` as input, and returns a tensor of shape `(batch_size, num_channels, height, width)`\n",
        "\n",
        "The network is built up as follows:\n",
        "* first, a convolutional layer is applied on the batch of noisy images, and position embeddings are computed for the noise levels\n",
        "* next, a sequence of downsampling stages are applied. Each downsampling stage consists of 2 ResNet/ConvNeXT blocks + groupnorm + attention + residual connection + a downsample operation\n",
        "* at the middle of the network, again ResNet or ConvNeXT blocks are applied, interleaved with attention\n",
        "* next, a sequence of upsampling stages are applied. Each upsampling stage consists of 2 ResNet/ConvNeXT blocks + groupnorm + attention + residual connection + an upsample operation\n",
        "* finally, a ResNet/ConvNeXT block followed by a convolutional layer is applied.\n",
        "\n",
        "Ultimately, neural networks stack up layers as if they were lego blocks (but it's important to [understand how they work](http://karpathy.github.io/2019/04/25/recipe/)).\n"
      ],
      "metadata": {
        "id": "teosKIpU7zYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        init_dim=None,\n",
        "        out_dim=None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        channels=3,\n",
        "        with_time_emb=True,\n",
        "        resnet_block_groups=8,\n",
        "        use_convnext=True,\n",
        "        convnext_mult=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # determine dimensions\n",
        "        self.channels = channels\n",
        "\n",
        "        init_dim = default(init_dim, dim // 3 * 2)\n",
        "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
        "\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        if use_convnext:\n",
        "            block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
        "        else:\n",
        "            block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
        "\n",
        "        # time embeddings\n",
        "        if with_time_emb:\n",
        "            time_dim = dim * 4\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEmbeddings(dim),\n",
        "                nn.Linear(dim, time_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(time_dim, time_dim),\n",
        "            )\n",
        "        else:\n",
        "            time_dim = None\n",
        "            self.time_mlp = None\n",
        "\n",
        "        # layers\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                        Downsample(dim_out) if not is_last else nn.Identity(),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.ups.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                        Upsample(dim_in) if not is_last else nn.Identity(),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        out_dim = default(out_dim, channels)\n",
        "        self.final_conv = nn.Sequential(\n",
        "            block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
        "\n",
        "        h = []\n",
        "\n",
        "        # downsample\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        # bottleneck\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        # upsample\n",
        "        for block1, block2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            x = upsample(x)\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "6rQGdKvf731f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the forward diffusion process\n",
        "\n",
        "The forward diffusion process gradually adds noise to an image from the real distribution, in a number of time steps $T$. This happens according to a **variance schedule**. The original DDPM authors employed a linear schedule:\n",
        "\n",
        "> We set the forward process variances to constants\n",
        "increasing linearly from $\\beta_1 = 10^{âˆ’4}$\n",
        "to $\\beta_T = 0.02$.\n",
        "\n",
        "However, it was shown in ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)) that better results can be achieved when employing a cosine schedule.\n",
        "\n",
        "Below, we define various schedules for the $T$ timesteps, as well as corresponding variables which we'll need, such as cumulative variances."
      ],
      "metadata": {
        "id": "SQBFRjG576v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "def linear_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "def quadratic_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(-6, 6, timesteps)\n",
        "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start"
      ],
      "metadata": {
        "id": "_0OPn28i77tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start with, let's use the linear schedule for \\\\(T=200\\\\) time steps and define the various variables from the \\\\(\\beta_t\\\\) which we will need, such as the cumulative product of the variances \\\\(\\bar{\\alpha}_t\\\\). Each of the variables below are just 1-dimensional tensors, storing values from \\\\(t\\\\) to \\\\(T\\\\). Importantly, we also define an `extract` function, which will allow us to extract the appropriate \\\\(t\\\\) index for a batch of indices.\n"
      ],
      "metadata": {
        "id": "ETlieVRK7-7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = 200\n",
        "\n",
        "# define beta schedule\n",
        "betas = linear_beta_schedule(timesteps=timesteps)\n",
        "\n",
        "# define alphas\n",
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "\n",
        "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "\n",
        "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    batch_size = t.shape[0]\n",
        "    out = a.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
      ],
      "metadata": {
        "id": "U_Nov5NK8CKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image"
      ],
      "metadata": {
        "id": "AeYWUM4r8GGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=17FXnvCTl96lDhqZ_io54guXO8hM-rsQ2\" width=\"400\" />\n",
        "\n",
        "Noise is added to PyTorch tensors, rather than Pillow Images. We'll first define image transformations that allow us to go from a PIL image to a PyTorch tensor (on which we can add the noise), and vice versa.\n",
        "\n",
        "These transformations are fairly simple: we first normalize images by dividing by $255$ (such that they are in the $[0,1]$ range), and then make sure they are in the $[-1, 1]$ range. From the DPPM paper:\n",
        "\n",
        "> We assume that image data consists of integers in $\\{0, 1, ... , 255\\}$ scaled linearly to $[âˆ’1, 1]$. This\n",
        "ensures that the neural network reverse process operates on consistently scaled inputs starting from\n",
        "the standard normal prior $p(\\mathbf{x}_T )$.\n"
      ],
      "metadata": {
        "id": "Kcl9ZroS8I7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
        "\n",
        "image_size = 128\n",
        "transform = Compose([\n",
        "    Resize(image_size),\n",
        "    CenterCrop(image_size),\n",
        "    ToTensor(), # turn into Numpy array of shape HWC, divide by 255\n",
        "    Lambda(lambda t: (t * 2) - 1),\n",
        "\n",
        "])\n",
        "\n",
        "x_start = transform(image).unsqueeze(0)\n",
        "x_start.shape"
      ],
      "metadata": {
        "id": "x11Yd6Um8LP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "reverse_transform = Compose([\n",
        "     Lambda(lambda t: (t + 1) / 2),\n",
        "     Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "     Lambda(lambda t: t * 255.),\n",
        "     Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "     ToPILImage(),\n",
        "])"
      ],
      "metadata": {
        "id": "vkqsnVxX8Ndy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_transform(x_start.squeeze())"
      ],
      "metadata": {
        "id": "3FJWTxJE8QGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward diffusion\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
        "    )\n",
        "\n",
        "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise"
      ],
      "metadata": {
        "id": "OF-xWMUb8SSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_noisy_image(x_start, t):\n",
        "  # add noise\n",
        "  x_noisy = q_sample(x_start, t=t)\n",
        "\n",
        "  # turn back into PIL image\n",
        "  noisy_image = reverse_transform(x_noisy.squeeze())\n",
        "\n",
        "  return noisy_image"
      ],
      "metadata": {
        "id": "9fvfHjTH8Uf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take time step\n",
        "t = torch.tensor([40])\n",
        "\n",
        "get_noisy_image(x_start, t)"
      ],
      "metadata": {
        "id": "3mAK5cSG8WKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisation"
      ],
      "metadata": {
        "id": "IAc3dVac8ZV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# use seed for reproducability\n",
        "torch.manual_seed(0)\n",
        "\n",
        "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0]) + with_orig\n",
        "    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        row = [image] + row if with_orig else row\n",
        "        for col_idx, img in enumerate(row):\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    if with_orig:\n",
        "        axs[0, 0].set(title='Original image')\n",
        "        axs[0, 0].title.set_size(8)\n",
        "    if row_title is not None:\n",
        "        for row_idx in range(num_rows):\n",
        "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "mKOx4Iks8bMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])"
      ],
      "metadata": {
        "id": "UlZbvjDV8dsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
        "    predicted_noise = denoise_model(x_noisy, t)\n",
        "\n",
        "    if loss_type == 'l1':\n",
        "        loss = F.l1_loss(noise, predicted_noise)\n",
        "    elif loss_type == 'l2':\n",
        "        loss = F.mse_loss(noise, predicted_noise)\n",
        "    elif loss_type == \"huber\":\n",
        "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "f5njz1YE8gGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `denoise_model` will be our U-Net defined above. We'll employ the Huber loss between the true and the predicted noise.\n",
        "\n",
        "## Define a PyTorch Dataset + DataLoader\n",
        "\n",
        "Here we define a regular [PyTorch Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). The dataset simply consists of images from a real dataset, like Fashion-MNIST, CIFAR-10 or ImageNet, scaled linearly to \\\\([âˆ’1, 1]\\\\).\n",
        "\n",
        "Each image is resized to the same size. Interesting to note is that images are also randomly horizontally flipped. From the paper:\n",
        "\n",
        "> We used random horizontal flips during training for CIFAR10; we tried training both with and without flips, and found flips to improve sample quality slightly.\n",
        "\n",
        "Here we use the ðŸ¤— [Datasets library](https://huggingface.co/docs/datasets/index) to easily load the Fashion MNIST dataset from the [hub](https://huggingface.co/datasets/fashion_mnist). This dataset consists of images which already have the same resolution, namely 28x28."
      ],
      "metadata": {
        "id": "aoV5ZlVO8jGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load dataset from the hub\n",
        "dataset = load_dataset(\"fashion_mnist\")\n",
        "image_size = 28\n",
        "channels = 1\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "ajXSNQSb8mUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define a function which we'll apply on-the-fly on the entire dataset. We use the `with_transform` [functionality](https://huggingface.co/docs/datasets/v2.2.1/en/package_reference/main_classes#datasets.Dataset.with_transform) for that. The function just applies some basic image preprocessing: random horizontal flips, rescaling and finally make them have values in the $[-1,1]$ range."
      ],
      "metadata": {
        "id": "E0-4MWlC8ony"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# define image transformations (e.g. using torchvision)\n",
        "transform = Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "])\n",
        "\n",
        "# define function\n",
        "def transforms(examples):\n",
        "   examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
        "   del examples[\"image\"]\n",
        "\n",
        "   return examples\n",
        "\n",
        "transformed_dataset = dataset.with_transform(transforms).remove_columns(\"label\")\n",
        "\n",
        "# create dataloader\n",
        "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ZzOmAsek8rBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(dataloader))\n",
        "print(batch.keys())"
      ],
      "metadata": {
        "id": "6R5aTbId8tgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling\n",
        "\n",
        "As we'll sample from the model during training (in order to track progress), we define the code for that below. Sampling is summarized in the paper as Algorithm 2:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ij80f8TNBDzpKtqHjk_sh8o5aby3lmD7\" width=\"500\" />\n",
        "\n",
        "Generating new images from a diffusion model happens by reversing the diffusion process: we start from $T$, where we sample pure noise from a Gaussian distribution, and then use our neural network to gradually denoise it (using the conditional probability it has learned), until we end up at time step $t = 0$. As shown above, we can derive a slighly less denoised image $\\mathbf{x}_{t-1 }$ by plugging in the reparametrization of the mean, using our noise predictor. Remember that the variance is known ahead of time.\n",
        "\n",
        "Ideally, we end up with an image that looks like it came from the real data distribution.\n",
        "\n",
        "The code below implements this."
      ],
      "metadata": {
        "id": "VD-jdlq38wBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, t_index):\n",
        "    betas_t = extract(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "    )\n",
        "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
        "\n",
        "    # Equation 11 in the paper\n",
        "    # Use our model (noise predictor) to predict the mean\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "\n",
        "    if t_index == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "        noise = torch.randn_like(x)\n",
        "        # Algorithm 2 line 4:\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "# Algorithm 2 but save all images:\n",
        "@torch.no_grad()\n",
        "def p_sample_loop(model, shape):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    b = shape[0]\n",
        "    # start from pure noise (for each example in the batch)\n",
        "    img = torch.randn(shape, device=device)\n",
        "    imgs = []\n",
        "\n",
        "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
        "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
        "        imgs.append(img.cpu().numpy())\n",
        "    return imgs\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, image_size, batch_size=16, channels=3):\n",
        "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))"
      ],
      "metadata": {
        "id": "aipZ4Hxi8ytF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note that the code above is a simplified version of the original implementation. We found our simplification (which is in line with Algorithm 2 in the paper) to work just as well as the [original, more complex implementation](https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/diffusion_utils.py).\n",
        "\n",
        "\n",
        "## Train the model\n",
        "\n",
        "Next, we train the model in regular PyTorch fashion. We also define some logic to peridiocally save generated images, using the `sample` method defined above.\n"
      ],
      "metadata": {
        "id": "Kj_rrrqD81sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "results_folder = Path(\"./results\")\n",
        "results_folder.mkdir(exist_ok = True)\n",
        "save_and_sample_every = 1000"
      ],
      "metadata": {
        "id": "cRqWPL5D84J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we define the model, and move it to the GPU. We also define a standard optimizer (Adam)."
      ],
      "metadata": {
        "id": "vh9j4Tb785LB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = Unet(\n",
        "    dim=image_size,\n",
        "    channels=channels,\n",
        "    dim_mults=(1, 2, 4,)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "XoycgIcA88B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4AvGULvp8_01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_size = batch[\"pixel_values\"].shape[0]\n",
        "      batch = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "      # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
        "      t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
        "\n",
        "      loss = p_losses(model, batch, t, loss_type=\"huber\")\n",
        "\n",
        "      if step % 100 == 0:\n",
        "        print(\"Loss:\", loss.item())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # save generated images\n",
        "      if step != 0 and step % save_and_sample_every == 0:\n",
        "        milestone = step // save_and_sample_every\n",
        "        batches = num_to_groups(4, batch_size)\n",
        "        all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))\n",
        "        all_images = torch.cat(all_images_list, dim=0)\n",
        "        all_images = (all_images + 1) * 0.5\n",
        "        save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)"
      ],
      "metadata": {
        "id": "EwHVvdLz9B9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling (inference)\n",
        "\n",
        "To sample from the model, we can just use our sample function defined above:\n"
      ],
      "metadata": {
        "id": "m0TlbWYo9E5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample 64 images\n",
        "samples = sample(model, image_size=image_size, batch_size=64, channels=channels)"
      ],
      "metadata": {
        "id": "L4Dd9onS9G79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show a random one\n",
        "random_index = 5\n",
        "plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap=\"gray\")"
      ],
      "metadata": {
        "id": "d8STxiaM9Irj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems like the model is capable of generating a nice T-shirt! Keep in mind that the dataset we trained on is pretty low-resolution (28x28).\n",
        "\n",
        "We can also create a gif of the denoising process:"
      ],
      "metadata": {
        "id": "ZnVYmdXp9KtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.animation as animation\n",
        "\n",
        "random_index = 53\n",
        "\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for i in range(timesteps):\n",
        "    im = plt.imshow(samples[i][random_index].reshape(image_size, image_size, channels), cmap=\"gray\", animated=True)\n",
        "    ims.append([im])\n",
        "\n",
        "animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
        "animate.save('diffusion.gif')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "38j8QB7g9NJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `diffusers`"
      ],
      "metadata": {
        "id": "2NvPpOgA9ZZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers==0.11.1"
      ],
      "metadata": {
        "id": "RnoG8jN19caM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview"
      ],
      "metadata": {
        "id": "KYWHSc9P9gB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One goal of the diffusers library is to make diffusion models accessible to a wide range of deep learning practitioners.\n",
        "With this in mind, we aimed at building a library that is **easy to use**, **intuitive to understand**, and **easy to contribute to**.\n",
        "\n",
        "As a quick recap, diffusion models are machine learning systems that are trained to *denoise* random gaussian noise step by step, to get to a sample of interest, such as an *image*.\n",
        "\n",
        "The underlying model, often a neural network, is trained to predict a way to slightly denoise the image in each step. After certain number of steps, a sample is obtained.\n",
        "\n",
        "The process is illustrated by the following design:\n",
        "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusion-process.png)"
      ],
      "metadata": {
        "id": "QdkjGznk9koI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The architecture of the neural network, referred to as **model**, commonly follows the UNet architecture as proposed in [this paper](https://arxiv.org/abs/1505.04597) and improved upon in the Pixel++ paper.\n",
        "\n",
        "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/unet-model.png)\n",
        "\n",
        "No worries if you don't understand everything. Some of the highlights of the architecture are:\n",
        "* this model predicts images of the same size as the input\n",
        "* the model makes the input image go through several blocks of ResNet layers which halves the image size by 2\n",
        "* then through the same number of blocks that upsample it again.\n",
        "* skip connections link features on the downsample path to corresponding layers in the upsample path.\n"
      ],
      "metadata": {
        "id": "LGRGbqJb9t2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The diffusion process consists in taking random noise of the size of the desired output and pass it through the model several times. The process ends after a given number of steps, and the output image should represent a sample according to the training data distribution of the model, for instance an image of a butterfly.\n",
        "\n",
        "During training we show many samples of a given distribution, such as images of butterfly. After training, the model will be able to process random noise to generate similar butterfly images.\n",
        "\n",
        "Without going in too much detail, the model is usually not trained to directly predict a slightly less noisy image, but rather to predict the \"noise residual\" which is the difference between a less noisy image and the input image (for a diffusion model called \"DDPM\") or, similarly, the gradient between the two time steps (like the diffusion model called \"Score VE\").\n",
        "\n",
        "To do the denoising process, a specific noise scheduling algorithm is thus necessary and \"wrap\" the model to define how many diffusion steps are needed for inference as well as how to *compute* a less noisy image from the model's output. Here is where the different **schedulers** of the diffusers library come into play.\n",
        "\n",
        "Finally, a **pipeline** groups together a **model** and a **scheduler** and make it easy for an end-user to run a full denoising loop process. We'll start with the pipelines and dive deeper into its implementation before taking a closer look at models and schedulers."
      ],
      "metadata": {
        "id": "jOZNnwKu9zzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core API"
      ],
      "metadata": {
        "id": "OpW3Oc16945M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipelines"
      ],
      "metadata": {
        "id": "hy-3KuZ995sU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin by importing a pipeline. We'll use the `google/ddpm-celebahq-256` model, built in collaboration by Google and U.C.Berkeley. It's a model following the [Denoising Diffusion Probabilistic Models (DDPM) algorithm](https://arxiv.org/abs/2006.11239) trained on a dataset of celebrities images.\n",
        "\n",
        "We can import the `DDPMPipeline`, which will allow you to do inference with a couple of lines of code:"
      ],
      "metadata": {
        "id": "tNvfnzZ099e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DDPMPipeline"
      ],
      "metadata": {
        "id": "OoYl1W-b-EoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `from_pretrained()` method allows downloading the model and its configuration from [the Hugging Face Hub](https://huggingface.co/google/ddpm-celebahq-256), a repository of over 60,000 models shared by the community.\n"
      ],
      "metadata": {
        "id": "Kf6ygQrW-BVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\n",
        "image_pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "B4lVyGAC-N3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate an image, we simply run the pipeline and don't even need to give it any input, it will generate a random initial noise sample and then iterate the diffusion process.\n",
        "\n",
        "The pipeline returns as output a dictionary with a generated `sample` of interest. This will typically take 2-3 minutes on Google Colab:"
      ],
      "metadata": {
        "id": "oPmV0CQv-O9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = image_pipe().images"
      ],
      "metadata": {
        "id": "fY56R1XV-S2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look ðŸ™‚"
      ],
      "metadata": {
        "id": "ZjwXGXGW-WP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "SDjB-yk9-aBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks pretty good!\n",
        "\n",
        "Now, let's try to understand a bit better what was going on under the hood. Let's see what the pipeline is made of:"
      ],
      "metadata": {
        "id": "nFtiPxi7-dwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_pipe"
      ],
      "metadata": {
        "id": "lFVR5f1U-mEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see inside the pipeline a scheduler and a UNet model. Let's have a closer look at them and what this pipeline just did behind the scenes."
      ],
      "metadata": {
        "id": "Bcvd57dv-qdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models\n",
        "\n",
        "Instances of the model class are neural networks that take a noisy `sample` as well as a `timestep` as inputs to predict a less noisy output `sample`. Let's load a pre-trained model and play around with it to understand the model API!\n",
        "\n",
        "We'll load a simple unconditional image generation model of type `UNet2DModel` which was released with the [DDPM Paper](https://arxiv.org/abs/2006.11239) and for instance take a look at another checkpoint trained on church images: [`google/ddpm-church-256`](https://huggingface.co/google/ddpm-church-256).\n",
        "\n",
        "Similarly to what we've seen for the pipeline class, we can load the model configuration and weights with one line, using the `from_pretrained()` method that you may be familiar with if you've played with the `transformers` library:"
      ],
      "metadata": {
        "id": "QSmNi_Ph-rWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "repo_id = \"google/ddpm-church-256\"\n",
        "model = UNet2DModel.from_pretrained(repo_id)"
      ],
      "metadata": {
        "id": "bu-_fb8z-jQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `from_pretrained()` method caches the model weights locally, so if you execute the cell above a second time, it will go much faster. The model is a pure PyTorch `torch.nn.Module` class which you can see when printing out `model`."
      ],
      "metadata": {
        "id": "irn3UFId-yTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "gDKiYnTs-1O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take a look at the model's configuration. The configuration can be accessed via the `config` attribute and shows all the necessary parameters to define the model architecture (and only those)."
      ],
      "metadata": {
        "id": "TXZf7A6w-33J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "XlUAL8j5-7_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model config is a frozen dictionary. This is to enforce that the configuration will **only** be used to define the model architecture at instantiation time and not for any attributes that can be changed during inference.\n",
        "\n",
        "A couple of important config parameters are:\n",
        "- `sample_size`: defines the `height` and `width` dimension of the input sample.\n",
        "- `in_channels`: defines the number of input channels of the input sample.\n",
        "- `down_block_types` and `up_block_types`: define the type of down- and upsampling blocks that are used to create the UNet architecture as was seen in the figure at the beginning of this notebook.\n",
        "- `block_out_channels`: defines the number of output channels of the downsampling blocks, also used in reversed order for the number of input channels of the upsampling blocks.\n",
        "- `layers_per_block`: defines how many ResNet blocks are present in each UNet block.\n",
        "\n",
        "Knowing how a UNet config looks like, you can quickly try to instantiate the exact same model architecture with random weights. To do so, let's pass the config as an unpacked dict to the `UNet2DModel` class."
      ],
      "metadata": {
        "id": "9NNDL1kJ-_Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_random = UNet2DModel(**model.config)"
      ],
      "metadata": {
        "id": "fE1mtIB2_H_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, the above created a randomly initialized model with the same config as the previous one."
      ],
      "metadata": {
        "id": "6VLu92ur_JNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to save the model you just created, you can use the `save_pretrained()` method, which saves both the model weights as well as the model config in the provided folder."
      ],
      "metadata": {
        "id": "fFmzyNYe_MlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_random.save_pretrained(\"my_model\")"
      ],
      "metadata": {
        "id": "IV0DbSi7_VYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at what files were saved in `my_model`."
      ],
      "metadata": {
        "id": "lfGJ1Qw3_Y73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls my_model"
      ],
      "metadata": {
        "id": "bkkAfaap_Zxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`diffusion_pytorch_model.bin` is a binary PyTorch file that stores the model weights and `config.json` stores the model's configuration."
      ],
      "metadata": {
        "id": "Fel4oU4W_dOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to reuse the model, you can simply use the `from_pretrained()` method again, as it loads local checkpoints as well as those present on the Hub."
      ],
      "metadata": {
        "id": "IymGF7Lt_pmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_random = UNet2DModel.from_pretrained(\"my_model\")"
      ],
      "metadata": {
        "id": "StV8EaK2_l5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coming back to the actually trained model, let's now see how you can use the model for inference. First, you need a random gaussian sample in the shape of an image (`batch_size` $\\times$ `in_channels` $\\times$ `sample_size` $\\times$ `sample_size`). We have a `batch` axis because a model can receive multiple random noises, a `channel` axis because each one consists of multiple channels (such as red-green-blue), and finally `sample_size` corresponds to the height and width."
      ],
      "metadata": {
        "id": "iUNdvIRV_y2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "noisy_sample = torch.randn(\n",
        "    1, model.config.in_channels, model.config.sample_size, model.config.sample_size\n",
        ")\n",
        "noisy_sample.shape"
      ],
      "metadata": {
        "id": "rWPHO03W_1w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to do the inference!\n",
        "\n",
        "You can pass the noisy sample alongside a `timestep` through the model. The timestep is important to cue the model with \"how noisy\" the input image is (more noisy in the beginning of the process, less noisy at the end), so the model knows if it's closer to the start or the end of the diffusion process.\n",
        "\n",
        "As explained in the introduction, the model predicts either the slightly less noisy image, the difference between the slightly less noisy image and the input image or even something else. It is important to carefully read through the [model card](https://huggingface.co/google/ddpm-church-256) to know what the model has been trained on. In this case, the model predicts the noise residual (difference between the slightly less noisy image and the input image)."
      ],
      "metadata": {
        "id": "dHjR_6cS_2hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    noisy_residual = model(sample=noisy_sample, timestep=2).sample"
      ],
      "metadata": {
        "id": "TKoGxuP2_5_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predicted `noisy_residual` has the exact same shape as the input and we use it to compute a slightly less noised image. Let's confirm the output shapes match:"
      ],
      "metadata": {
        "id": "YEEuuDOS_-Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noisy_residual.shape"
      ],
      "metadata": {
        "id": "F53ZbZNzABPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great.\n",
        "\n",
        "Now to summarize, **models**, such as `UNet2DModel` (PyTorch modules) are parameterized neural networks trained to *predict* a slightly less noisy image or residual. They are defined by their `.config` and can be loaded from the Hub as well as saved and loaded locally. The next step is learning how to combine this **model** with the correct **scheduler** to be able to actually generate images."
      ],
      "metadata": {
        "id": "AmQJf8hnAEJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Schedulers\n",
        "\n",
        "**Schedulers** are algorithms wrapped into a Python class. They define the noise schedule which is used to add noise to the model during training, and also define the algorithm to *compute* the slightly less noisy sample given the model output (here `noisy_residual`). This notebook focuses only on how to use *scheduler* classes for inference. You can check out this notebook to see how to use *schedulers* for training.\n",
        "\n",
        "It is important to stress here that while *models* have trainable weights, *schedulers* are usually *parameter-free* (in the sense they have no trainable weights) and simply define the algorithm to compute the slightly less noisy sample. Schedulers thus don't inherit from `torch.nn.Module`, but like models they are instantiated by a configuration.\n",
        "\n",
        "To download a scheduler config from the Hub, you can make use of the `from_config()` method to load a configuration and instantiate a scheduler.\n",
        "\n",
        "Let's use `DDPMScheduler`, the denoising algorithm proposed in the [DDPM Paper](https://arxiv.org/abs/2006.11239)."
      ],
      "metadata": {
        "id": "wFvIFYj-ALST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DDPMScheduler\n",
        "\n",
        "scheduler = DDPMScheduler.from_config(repo_id)"
      ],
      "metadata": {
        "id": "tgwkeuOyAHxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also take a look at the config here"
      ],
      "metadata": {
        "id": "fyAWKFIzAQDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler.config"
      ],
      "metadata": {
        "id": "VN1-msftATlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different schedulers are usually defined by different parameters. To better understand what the parameters are used for exactly, the reader is advised to directly look into the respective scheduler files under `src/diffusers/schedulers/`, such as the [`src/diffusers/schedulers/scheduling_ddpm.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py) file. Here are the most important ones:\n",
        "- `num_train_timesteps` defines the length of the denoising process, e.g. how many timesteps are need to process random gaussian noise to a data sample.\n",
        "- `beta_schedule` define the type of noise schedule that shall be used for inference and training\n",
        "- `beta_start` and `beta_end` define the smallest noise value and highest noise value of the schedule.\n",
        "\n",
        "Like the *models*, *schedulers* can be saved and loaded with `save_config()` and `from_config()`.\n"
      ],
      "metadata": {
        "id": "mahOUEK0AWUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler.save_config(\"my_scheduler\")\n",
        "new_scheduler = DDPMScheduler.from_config(\"my_scheduler\")"
      ],
      "metadata": {
        "id": "IFkITzSVAdY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All schedulers provide one or multiple `step()` methods that can be used to compute the slightly less noisy image. The `step()` method may vary from one scheduler to another, but normally expects at least the model output, the `timestep` and the current `noisy_sample`.\n",
        "\n",
        "Note that the `step()` method is somewhat of a black box function that \"just works\". If you are keen to better understand how exactly the previous noisy sample is computed as defined in the original paper of the scheduler, you should take a look at the actual code, *e.g.* [click here](https://github.com/huggingface/diffusers/blob/936cd08488260a9df3548d66628b83bc7f26bd9e/src/diffusers/schedulers/scheduling_ddpm.py#L130) for DDPM, which contains comments and references to the original paper.\n",
        "\n",
        "Let's give it a try using the model output from the previous section."
      ],
      "metadata": {
        "id": "noNkw3nUAeG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "less_noisy_sample = scheduler.step(\n",
        "    model_output=noisy_residual, timestep=2, sample=noisy_sample\n",
        ").prev_sample\n",
        "less_noisy_sample.shape"
      ],
      "metadata": {
        "id": "rYOV8ozXAj4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the computed sample has the exact same shape as the model input, meaning that you are ready to pass it to the model again in a next step.\n",
        "\n",
        "Let's now bring it all together and actually define the denoising loop. This loop prints out the (less and less) noisy samples along the way for better visualization in the denoising loop. Let's define a display function that takes care of post-processing the denoised image, convert it to a `PIL.Image` and displays it."
      ],
      "metadata": {
        "id": "ydsn41HCAksu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "import numpy as np\n",
        "\n",
        "def display_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Image at step {i}\")\n",
        "    display(image_pil)"
      ],
      "metadata": {
        "id": "3m8W3t3cAqFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before defining the loop, let's move the input and model to the GPU to speed up the denoising process a bit."
      ],
      "metadata": {
        "id": "HI1hwdXPAtUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cuda\")\n",
        "noisy_sample = noisy_sample.to(\"cuda\")"
      ],
      "metadata": {
        "id": "LymeQ0jKAubC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to finally define the denoising loop! It is rather straight-forward for DDPM.\n",
        "\n",
        "1. Predict the residual of the less noisy sample with the model.\n",
        "2. Compute the less noisy sample with the scheduler.\n",
        "\n",
        "Additionally, at every 50th step this will display the progress.\n",
        "\n",
        "It's important to note here that you loop over `scheduler.timesteps` which is a tensor defining the sequence of timesteps over which to iterate during the denoising process. Usually, the denoising process goes in decreasing order of timesteps, so from the total number of timesteps (here 1000) to 0.\n",
        "\n",
        "Depending on your GPU this might take up to a minute - enough time to reflect on everything you learned so far while you can watch a church being built from nothing but noise â›ª."
      ],
      "metadata": {
        "id": "Gcb9qzjIAxpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "sample = noisy_sample\n",
        "\n",
        "for i, t in enumerate(tqdm.tqdm(scheduler.timesteps)):\n",
        "  # 1. predict noise residual\n",
        "  with torch.no_grad():\n",
        "      residual = model(sample, t).sample\n",
        "\n",
        "  # 2. compute less noisy image and set x_t -> x_t-1\n",
        "  sample = scheduler.step(residual, t, sample).prev_sample\n",
        "\n",
        "  # 3. optionally look at image\n",
        "  if (i + 1) % 50 == 0:\n",
        "      display_sample(sample, i + 1)"
      ],
      "metadata": {
        "id": "wFTdnoE8A3Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that it takes quite some time to see a somewhat meaningful shape - only after *ca.* 800 steps.\n",
        "\n",
        "While the quality of the image is actually quite good - you might want to speed up the image generation.\n",
        "\n",
        "To do so, you can try replacing the DDPM scheduler with the [DDIM](https://arxiv.org/abs/2010.02502) scheduler which keep high generation quality at significantly sped-up generation time.\n",
        "\n",
        "**Exchanging schedulers**: one of the exciting prospects of a diffusion model library is that different scheduling protocols can work with different models, but there is not a one-sized fits all solution!\n",
        "In this case, DDIM worked as an swap for DDPM, but this not universal (and represents an interesting research problem).\n"
      ],
      "metadata": {
        "id": "fmHQDaBxBCqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DDIMScheduler\n",
        "\n",
        "scheduler = DDIMScheduler.from_config(repo_id)"
      ],
      "metadata": {
        "id": "DvaSl2UUBMuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DDIM scheduler allows the user to define how many denoising steps should be run at inference via the `set_timesteps` method. The DDPM scheduler runs by default 1000 denoising steps. Let's significantly reduce this number to just 50 inference steps for DDIM."
      ],
      "metadata": {
        "id": "Y3TQsGd_BNk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler.set_timesteps(num_inference_steps=50)"
      ],
      "metadata": {
        "id": "WeYO89U5BQiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And you can run the same loop as before - only that you are now making use of the much faster DDIM scheduler."
      ],
      "metadata": {
        "id": "-t9VdlgEBUCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "sample = noisy_sample\n",
        "\n",
        "for i, t in enumerate(tqdm.tqdm(scheduler.timesteps)):\n",
        "  # 1. predict noise residual\n",
        "  with torch.no_grad():\n",
        "      residual = model(sample, t).sample\n",
        "\n",
        "  # 2. compute previous image and set x_t -> x_t-1\n",
        "  sample = scheduler.step(residual, t, sample).prev_sample\n",
        "\n",
        "  # 3. optionally look at image\n",
        "  if (i + 1) % 10 == 0:\n",
        "      display_sample(sample, i + 1)"
      ],
      "metadata": {
        "id": "vPspjotkBXdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the image generation is indeed much faster - a mere two seconds - but also that you pay by giving away image quality in exchange for speed."
      ],
      "metadata": {
        "id": "0T4fViCgBfDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, now you should have gotten a good first understanding of the schedulers. The important things to remember are:\n",
        "1. schedulers are *parameter-free* (no trainable weights)\n",
        "2. schedulers define the algorithm computing the slightly less noisy sample during inference\n",
        "\n",
        "They are many schedulers already added to `diffusers` and diffusers will be adding even more in the future. It's important that you read the model cards to understand which model checkpoints can be used with which schedulers.\n",
        "You can find all available schedulers [here](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers)."
      ],
      "metadata": {
        "id": "pAui2zP9Biaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To end the chapter about **models** and **schedulers**, please also note that we very much *deliberately* try to keep *models* and *schedulers* as independent from each other as possible. This means a `scheduler` should never accept a `model` as an input and vice-versa. The model *predict* the noise residual or slightly less noisy image with its trained weights, while the scheduler *computes* the previous sample given the model's output."
      ],
      "metadata": {
        "id": "xPxPQseLBm2K"
      }
    }
  ]
}