{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahamsi/computer-vision/blob/main/week-9/Vidoe_Segmentation_with_SAM_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/tahamsi/computer-vision)"
      ],
      "metadata": {
        "id": "Vwd5eYCD05jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Segmentation with Segment Anything Model v2 (SAM 2)\n",
        "\n",
        "\n",
        "\n",
        "[Segment Anything Model 2](https://github.com/facebookresearch/sam2?tab=readme-ov-file) (SAM 2) is a foundation model designed for promptable visual segmentation in both images and videos. To extend SAM's capabilities to video, individual images are treated as single-frame videos. The model utilizes a streamlined transformer architecture with a streaming memory component to enable real-time video processing. Trained on a large, diverse dataset, SAM 2 demonstrates strong performance across various tasks and visual domains.\n",
        "\n",
        "![segment anything model](https://media.roboflow.com/notebooks/examples/segment-anything-model-2-paper.jpg)\n",
        "\n",
        "##Promptable Visial Segmentation\n",
        "Promptable visual segmentation is a technique where a segmentation model, such as SAM 2, is guided by prompts to identify and segment specific parts of an image or video. Prompts can be various types of inputs, such as:\n",
        "\n",
        "* Points: Locations in the image that indicate the starting position or an object to be segmented.\n",
        "* Boxes: Bounding boxes around objects, helping the model understand the region of interest.\n",
        "* Masks: Roughly defined regions, which the model can refine into more accurate segmentations.\n",
        "* Text Prompts: In some models, text descriptions guide the segmentation to certain types of objects (e.g., “segment the dog” in an image with multiple animals).\n",
        "\n",
        "The benefit of promptable segmentation is flexibility; users can direct the model to focus on specific areas or types of objects without pre-defined categories, making it adaptable for various tasks and contexts.\n",
        "\n"
      ],
      "metadata": {
        "id": "l2Es_L1iNX4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before you start\n",
        "\n",
        "Let's ensure you have `GPU` access by using the `nvidia-smi` command to check. If there are any issues, go to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. The codes here are extended from [Meta AI](https://github.com/facebookresearch/sam2?tab=readme-ov-file) and [Roboflow](https://github.com/roboflow/notebooks)."
      ],
      "metadata": {
        "id": "DSffnnWDNhb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA9bQMozM_wg"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install SAM2"
      ],
      "metadata": {
        "id": "yo5LAKqyNzfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "!git clone https://github.com/facebookresearch/sam2.git\n",
        "%cd sam2\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "CBrYxp6nNpqk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q supervision[assets] jupyter_bbox_widget"
      ],
      "metadata": {
        "id": "UGv98ypgPA7c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download checkpoints\n",
        "\n",
        "SAM2 is available in four model sizes, ranging from the lightweight `sam2_hiera_tiny` with 38.9 million parameters to the more powerful `sam2_hiera_large` with 224.4 million parameters."
      ],
      "metadata": {
        "id": "g3Psmg3sOzIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P {HOME}/checkpoints"
      ],
      "metadata": {
        "id": "Dq_DR0IJN_1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "BHRLQPV4WKd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import base64\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from pathlib import Path\n",
        "from supervision.assets import download_assets, VideoAssets\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "IS_COLAB = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "from jupyter_bbox_widget import BBoxWidget"
      ],
      "metadata": {
        "id": "vIcNq3IiXufS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code enables `mixed-precision` computing to accelerate deep learning. It primarily uses bfloat16 for calculations and, on newer NVIDIA GPUs, leverages TensorFloat-32 (TF32) for specific operations to enhance performance further."
      ],
      "metadata": {
        "id": "svThmVIGZZAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "MZAXCVT0ZWkd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NLeXwS2UQU5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT = f\"{HOME}/checkpoints/sam2_hiera_large.pt\"\n",
        "CONFIG = \"sam2_hiera_l.yaml\"\n",
        "\n",
        "sam2_model = build_sam2_video_predictor(CONFIG, CHECKPOINT)"
      ],
      "metadata": {
        "id": "xHvgsf08QRZo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess video"
      ],
      "metadata": {
        "id": "vIA7L7FoVWqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download video and split it into frames\n",
        "\n",
        "SAM2 assumes that the video is stored as a series of `JPEG` frames with filenames formatted as `<frame_index>.jpg`. To get started, let’s download a sample video, split it into frames, and save them to disk. You can replace `SOURCE_VIDEO` with the path to your own video file."
      ],
      "metadata": {
        "id": "O6siyjFOAmCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_VIDEO = download_assets(VideoAssets.BASKETBALL)\n",
        "sv.VideoInfo.from_video_path(SOURCE_VIDEO)"
      ],
      "metadata": {
        "id": "72oiBQYvUSws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reduce VRAM usage, there are three additional parameters: `SCALE_FACTOR` to lower the frame resolution, and `START_IDX` and `END_IDX` to extract only specific segments of the video."
      ],
      "metadata": {
        "id": "ht5NQoQkCXup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCALE_FACTOR = 0.5\n",
        "START_IDX = 100\n",
        "END_IDX = 300"
      ],
      "metadata": {
        "id": "1gjiF_6gV1By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_FRAMES = Path(HOME) / Path(SOURCE_VIDEO).stem\n",
        "SOURCE_FRAMES.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "frames_generator = sv.get_video_frames_generator(SOURCE_VIDEO, start=START_IDX, end=END_IDX)\n",
        "images_sink = sv.ImageSink(\n",
        "    target_dir_path=SOURCE_FRAMES.as_posix(),\n",
        "    overwrite=True,\n",
        "    image_name_pattern=\"{:05d}.jpeg\"\n",
        ")\n",
        "\n",
        "with images_sink:\n",
        "    for frame in frames_generator:\n",
        "        frame = sv.scale_image(frame, SCALE_FACTOR)\n",
        "        images_sink.save_image(frame)\n",
        "\n",
        "TARGET_VIDEO = Path(HOME) / f\"{Path(SOURCE_VIDEO).stem}-result.mp4\"\n",
        "SOURCE_FRAME_PATHS = sorted(sv.list_files_with_extensions(SOURCE_FRAMES.as_posix(), extensions=[\"jpeg\"]))"
      ],
      "metadata": {
        "id": "Cq76K2btCpI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the inference state\n",
        "\n",
        "SAM 2 requires stateful inference for interactive video segmentation, which means you need to initialize an inference state for this video. During initialization, SAM 2 loads all JPEG frames from `video_path` and stores their pixel data in `inference_state`, as indicated in the progress bar below."
      ],
      "metadata": {
        "id": "kti4CAf8CwUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_state = sam2_model.init_state(video_path=SOURCE_FRAMES.as_posix())"
      ],
      "metadata": {
        "id": "iRgZK5VsDCw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have used this `inference_state` for any previous tracking, please reset it first using reset_state. (The cell below is for illustration only; there’s no need to call `reset_state` here, as this `inference_state` has just been freshly initialized above.)"
      ],
      "metadata": {
        "id": "JE9c3pILDDuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sam2_model.reset_state(inference_state)"
      ],
      "metadata": {
        "id": "we90u0PSWUeq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting with points"
      ],
      "metadata": {
        "id": "K2YhLwHiwOnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute cell below and use your mouse to **draw points** on the image."
      ],
      "metadata": {
        "id": "ex_EELQswkZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(filepath):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        image_bytes = f.read()\n",
        "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
        "    return \"data:image/jpg;base64,\"+encoded"
      ],
      "metadata": {
        "id": "QdP1a04RD0bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAM2 supports tracking multiple objects simultaneously. To adjust the list of tracked objects, simply update the `OBJECTS` list."
      ],
      "metadata": {
        "id": "kP69vj-ND2XQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OBJECTS = ['ball', 'player-1', 'player-2']"
      ],
      "metadata": {
        "id": "LvhZtqzcv1ZH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the index of the reference frame that will be used to annotate the objects of interest."
      ],
      "metadata": {
        "id": "X_IZZSLpES80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FRAME_IDX = 0\n",
        "FRAME_PATH = Path(SOURCE_FRAMES) / f\"{FRAME_IDX:05d}.jpeg\"\n",
        "\n",
        "widget = BBoxWidget(classes=OBJECTS)\n",
        "widget.image = encode_image(FRAME_PATH)\n",
        "widget"
      ],
      "metadata": {
        "id": "biiUhip93tol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The widget you’re using stores annotations in a format that doesn’t align with SAM2's requirements. To address this, you can parse the annotations and pass them to SAM2 using the `add_new_points` method. Each object being tracked requires a separate `add_new_points` call. It’s essential to specify `frame_idx` each time —the index of the frame the annotations refer to— and `obj_id`, the ID of the object associated with the annotations."
      ],
      "metadata": {
        "id": "QqlnYRVAEqoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_box = [\n",
        "    {'x': 705, 'y': 302, 'width': 0, 'height': 0, 'label': 'ball'},\n",
        "    {'x': 587, 'y': 300, 'width': 0, 'height': 0, 'label': 'player-1'},\n",
        "    {'x': 753, 'y': 267, 'width': 0, 'height': 0, 'label': 'player-2'}\n",
        "]\n",
        "\n",
        "boxes = widget.bboxes if widget.bboxes else default_box\n",
        "\n",
        "for object_id, label in enumerate(OBJECTS, start=1):\n",
        "    boxes = [box for box in widget.bboxes if box['label'] == label]\n",
        "\n",
        "    if len(boxes) == 0:\n",
        "        continue\n",
        "\n",
        "    points = np.array([\n",
        "        [\n",
        "            box['x'],\n",
        "            box['y']\n",
        "        ] for box in boxes\n",
        "    ], dtype=np.float32)\n",
        "    labels = np.ones(len(points))\n",
        "\n",
        "    _, object_ids, mask_logits = sam2_model.add_new_points(\n",
        "        inference_state=inference_state,\n",
        "        frame_idx=FRAME_IDX,\n",
        "        obj_id=object_id,\n",
        "        points=points,\n",
        "        labels=labels,\n",
        "    )"
      ],
      "metadata": {
        "id": "xBc0Y_T139lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video inference\n",
        "\n",
        "**NOTE:** To apply the point prompts to all video frames, use the `propagate_in_video` generator. Each call returns `frame_idx` - the index of the current frame, `object_ids` - IDs of objects detected in the frame, and `mask_logits` - corresponding `object_ids` logit values, which you can convert to masks using thresholding."
      ],
      "metadata": {
        "id": "ynrlHQaEFZjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO)\n",
        "video_info.width = int(video_info.width * SCALE_FACTOR)\n",
        "video_info.height = int(video_info.height * SCALE_FACTOR)\n",
        "\n",
        "COLORS = ['#FF1493', '#00BFFF', '#FF6347', '#FFD700']\n",
        "mask_annotator = sv.MaskAnnotator(\n",
        "    color=sv.ColorPalette.from_hex(COLORS),\n",
        "    color_lookup=sv.ColorLookup.CLASS)\n",
        "\n",
        "frame_sample = []\n",
        "\n",
        "with sv.VideoSink(TARGET_VIDEO.as_posix(), video_info=video_info) as sink:\n",
        "    for frame_idx, object_ids, mask_logits in sam2_model.propagate_in_video(inference_state):\n",
        "        frame_path = SOURCE_FRAME_PATHS[frame_idx]\n",
        "        frame = cv2.imread(frame_path)\n",
        "        masks = (mask_logits > 0.0).cpu().numpy()\n",
        "        masks = np.squeeze(masks).astype(bool)\n",
        "\n",
        "        detections = sv.Detections(\n",
        "            xyxy=sv.mask_to_xyxy(masks=masks),\n",
        "            mask=masks,\n",
        "            class_id=np.array(object_ids)\n",
        "        )\n",
        "\n",
        "        annotated_frame = mask_annotator.annotate(scene=frame.copy(), detections=detections)\n",
        "\n",
        "        sink.write_frame(annotated_frame)\n",
        "        if frame_idx % video_info.fps == 0:\n",
        "            frame_sample.append(annotated_frame)"
      ],
      "metadata": {
        "id": "c6rS9seW4bbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sv.plot_images_grid(\n",
        "    images=frame_sample[:4],\n",
        "    grid_size=(2, 2)\n",
        ")"
      ],
      "metadata": {
        "id": "A5tueOO05MwC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}