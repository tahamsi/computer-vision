{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzKitd2RCiR0cQXSGKNgGc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahamsi/computer-vision/blob/main/week-10/Object_tracking_and_counting_with_yolo11_supervision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/tahamsi/computer-vision)"
      ],
      "metadata": {
        "id": "Ab3TbY5JPgbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Tracking\n",
        "\n",
        "\n",
        "**Object tracking** is a field within computer vision that involves the process of locating and following a specific object or multiple objects in a sequence of frames within a video. The primary goal of object tracking is to identify and trace the movement of objects over time as they move within a video or a series of consecutive frames.\n",
        "\n",
        "The process of object tracking typically involves the following steps:\n",
        "\n",
        "* Detection: Initially, an object detector or segmentation algorithm identifies and localizes objects within the first frame of the video sequence.\n",
        "\n",
        "* Initialization: Once the object is detected in the first frame, a bounding box or a specific region of interest around the object is defined, and its characteristics (such as appearance features, color, shape, or motion) are extracted to create a representation.\n",
        "\n",
        "* Tracking: Using the defined characteristics, the tracker continuously predicts the object's position or state in subsequent frames by updating and adjusting the initial representation. This is done by estimating the object's location, size, orientation, and other relevant attributes.\n",
        "\n",
        "* Updating: As the object moves, changes direction, or experiences occlusion, the tracking algorithm adapts to these variations, maintaining the object's trajectory and characteristics across frames.\n",
        "\n",
        "## YOLO\n",
        "YOLO (You Only Look Once) is a popular deep learning model used for real-time object detection. It was introduced by Joseph Redmon and is known for its ability to detect multiple objects in an image or video frame with high speed and accuracy. YOLO stands out due to its innovative approach of treating object detection as a single regression problem, enabling it to predict bounding boxes and class probabilities directly from full images in one evaluation.\n",
        "\n",
        "Key Features\n",
        "\n",
        "* Single-Pass Detection: Unlike traditional object detection methods that use a multi-stage process (e.g., region proposal and classification), YOLO processes an image in a single neural network pass, making it extremely fast and suitable for real-time applications.\n",
        "* Grid-Based Prediction: YOLO divides the input image into a grid and assigns each grid cell the responsibility of predicting bounding boxes and their associated class probabilities if the center of an object falls within that cell.\n",
        "* End-to-End Learning: The model is trained end-to-end, optimizing for both object localization and classification simultaneously.\n",
        "* Speed and Efficiency: YOLO is capable of processing images at high frame rates, making it suitable for applications that require real-time performance, such as video surveillance, autonomous vehicles, and interactive systems.\n",
        "\n",
        "## YOLO11\n",
        "[Ultralytics YOLO11](https://docs.ultralytics.com/modes/track/) is a state-of-the-art model that builds on the success of previous YOLO versions, incorporating new features and enhancements to further improve performance and flexibility. **YOLO11** is designed to be fast, accurate, and user-friendly, making it an ideal choice for a variety of tasks, including object detection, tracking, instance segmentation, image classification, and pose estimation.\n",
        "\n",
        "The output from Ultralytics trackers aligns with standard object detection while incorporating object IDs, enabling seamless object tracking in video streams and advanced analytics. Here’s why Ultralytics YOLO is an excellent choice for your object tracking needs:\n",
        "\n",
        "* **Efficiency**: Processes video streams in real-time with high accuracy.\n",
        "* **Flexibility**: Supports various tracking algorithms and configurable options to suit diverse use cases.\n",
        "* **Ease of Use**: Features a straightforward Python API and CLI for quick setup and deployment.\n",
        "* **Customizability**: Compatible with custom-trained YOLO models, making it ideal for domain-specific applications."
      ],
      "metadata": {
        "id": "qQeIsexnGd-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to `GPU`. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit -> Notebook settings -> Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "REo7t-olJNXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "6BZfkGkQJtom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install YOLO\n",
        "\n",
        "Install the Ultralytics package, along with all required dependencies, in a Python environment (version 3.8 or higher) with `PyTorch` (version 1.8 or higher) using the following command: `pip install ultralytics`."
      ],
      "metadata": {
        "id": "VvGk1sWHJ1m3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wo1dZAs6CZe3"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "!mkdir -p {HOME}/data"
      ],
      "metadata": {
        "id": "K8jRc5IECnVs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "UQu6LrGNLxVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tahamsi/computer-vision/refs/heads/main/images/people.mp4 -P {HOME}/data\n",
        "source_video = f\"{HOME}/data/people.mp4\"\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "WAEqmZDSCqn8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a model\n",
        "\n",
        "You can now choose the core model for your object tracking tasks: either object detection or instance segmentation as the base model. Whichever mode you select, the corresponding checkpoints will be automatically downloaded."
      ],
      "metadata": {
        "id": "hEMAgQBqMoCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolo11n-seg.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYmB7v9vDL4o",
        "outputId": "2c976193-227e-405e-81fe-53a52f13896c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-seg.pt to 'yolo11n-seg.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.90M/5.90M [00:00<00:00, 53.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ByteTrack\n",
        "\n",
        "In this implementation, we utilize **ByteTrack** as our object tracking model.\n",
        "\n",
        "[ByteTrack](https://github.com/ifzhang/ByteTrack) is a cutting-edge multi-object tracking (MOT) algorithm that enhances tracking accuracy by associating both high-confidence and low-confidence detections across frames. Unlike traditional trackers that often discard low-confidence detections, ByteTrack integrates them into the tracking process, which helps in handling occlusions, crowded scenes, and challenging environments more effectively. It relies on an advanced data association strategy that combines appearance and motion cues to robustly maintain object trajectories over time. ByteTrack achieves state-of-the-art performance on popular benchmarks, such as MOT Challenge, while remaining computationally efficient, making it suitable for real-world applications in surveillance, autonomous driving, and video analytics. Its versatility and robustness have made it a popular choice for multi-object tracking tasks.\n",
        "\n",
        "### Supervision\n",
        "You can either install and set up ByteTrack directly or use [Roboflow Supervision](https://github.com/roboflow/supervision), which conveniently packages it into an easy-to-use module.Supervision is designed to be model-agnostic, allowing seamless integration with any classification, detection, or segmentation model."
      ],
      "metadata": {
        "id": "muwUSKO3NgTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision\n",
        "\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "1PiZmuyuCvL6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import required libraries"
      ],
      "metadata": {
        "id": "311eDv1HQNbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jwo8jJrtC3q4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization\n",
        "\n",
        "Limit the model to focus exclusively on the objects of interest. Additionally, since the goal is to count moving objects, you can define a specific region in the video—preferably closer to the camera—to track and count these objects. To achieve this, the region's coordinates should be specified."
      ],
      "metadata": {
        "id": "A0_IgftyQTsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_nmaes = model.model.names\n",
        "# class_id of interest - person\n",
        "class_ids = [0]\n",
        "\n",
        "# settings\n",
        "start = sv.Point(50, 700)\n",
        "end = sv.Point(1700, 700)\n",
        "\n",
        "target_video = f\"{HOME}/result.mp4\""
      ],
      "metadata": {
        "id": "dWdDxorUDhv9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyse the input video. And create a VideoInfo instance"
      ],
      "metadata": {
        "id": "EyPxEIk4SGwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_info = sv.VideoInfo.from_video_path(source_video)"
      ],
      "metadata": {
        "id": "O-UxV-veD1pJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create `ByteTrack` instance."
      ],
      "metadata": {
        "id": "1x8nMEDESSWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create BYTETracker instance\n",
        "byte_tracker = sv.ByteTrack(\n",
        "    track_activation_threshold=0.25,\n",
        "    lost_track_buffer=30,\n",
        "    minimum_matching_threshold=0.8,\n",
        "    frame_rate=30,\n",
        "    minimum_consecutive_frames=3)\n",
        "\n",
        "byte_tracker.reset()"
      ],
      "metadata": {
        "id": "F1WBRdmeSUH0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect YOLO to ByteTrack"
      ],
      "metadata": {
        "id": "Iu9so1Z_S2cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create frame generator\n",
        "generator = sv.get_video_frames_generator(source_video)\n",
        "\n",
        "# create LineZone instance, it is previously called LineCounter class\n",
        "line_zone = sv.LineZone(start=start, end=end)\n",
        "\n",
        "# create instance of BoxAnnotator, LabelAnnotator, and TraceAnnotator\n",
        "box_annotator = sv.BoxAnnotator(thickness=4)\n",
        "label_annotator = sv.LabelAnnotator(text_thickness=2, text_scale=1.5, text_color=sv.Color.BLACK)\n",
        "trace_annotator = sv.TraceAnnotator(thickness=4, trace_length=50)\n",
        "\n",
        "# create LineZoneAnnotator instance, it is previously called LineCounterAnnotator class\n",
        "line_zone_annotator = sv.LineZoneAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
        "\n",
        "# define call back function to be used in video processing\n",
        "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
        "    # model prediction on single frame and conversion to supervision Detections\n",
        "    results = model(frame, verbose=False)[0]\n",
        "    detections = sv.Detections.from_ultralytics(results)\n",
        "    # only consider class id from selected_classes define above\n",
        "    detections = detections[np.isin(detections.class_id, class_ids)]\n",
        "    # tracking detections\n",
        "    detections = byte_tracker.update_with_detections(detections)\n",
        "    labels = [\n",
        "        f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"\n",
        "        for confidence, class_id, tracker_id\n",
        "        in zip(detections.confidence, detections.class_id, detections.tracker_id)\n",
        "    ]\n",
        "    annotated_frame = frame.copy()\n",
        "    annotated_frame = trace_annotator.annotate(\n",
        "        scene=annotated_frame, detections=detections)\n",
        "    annotated_frame = box_annotator.annotate(\n",
        "        scene=annotated_frame, detections=detections)\n",
        "    annotated_frame = label_annotator.annotate(\n",
        "        scene=annotated_frame, detections=detections, labels=labels)\n",
        "\n",
        "    # update line counter\n",
        "    line_zone.trigger(detections)\n",
        "    # return frame with box and line annotated result\n",
        "    return line_zone_annotator.annotate(annotated_frame, line_counter=line_zone)"
      ],
      "metadata": {
        "id": "eWfWRSaFD_YV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process the video"
      ],
      "metadata": {
        "id": "MCFXyLTqTLCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sv.process_video(\n",
        "    source_path = source_video,\n",
        "    target_path = target_video,\n",
        "    callback=callback\n",
        ")"
      ],
      "metadata": {
        "id": "GAfxbUxjTPlj"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}