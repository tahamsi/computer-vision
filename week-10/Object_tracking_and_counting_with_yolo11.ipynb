{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM/Lsvtf2wmUJj5HcgU0Df8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahamsi/computer-vision/blob/main/week-10/Object_tracking_and_counting_with_yolo11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/tahamsi/computer-vision)"
      ],
      "metadata": {
        "id": "Ab3TbY5JPgbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Tracking\n",
        "\n",
        "\n",
        "**Object tracking** is a field within computer vision that involves the process of locating and following a specific object or multiple objects in a sequence of frames within a video. The primary goal of object tracking is to identify and trace the movement of objects over time as they move within a video or a series of consecutive frames.\n",
        "\n",
        "The process of object tracking typically involves the following steps:\n",
        "\n",
        "* Detection: Initially, an object detector or segmentation algorithm identifies and localizes objects within the first frame of the video sequence.\n",
        "\n",
        "* Initialization: Once the object is detected in the first frame, a bounding box or a specific region of interest around the object is defined, and its characteristics (such as appearance features, color, shape, or motion) are extracted to create a representation.\n",
        "\n",
        "* Tracking: Using the defined characteristics, the tracker continuously predicts the object's position or state in subsequent frames by updating and adjusting the initial representation. This is done by estimating the object's location, size, orientation, and other relevant attributes.\n",
        "\n",
        "* Updating: As the object moves, changes direction, or experiences occlusion, the tracking algorithm adapts to these variations, maintaining the object's trajectory and characteristics across frames.\n",
        "\n",
        "## YOLO\n",
        "YOLO (You Only Look Once) is a popular deep learning model used for real-time object detection. It was introduced by Joseph Redmon and is known for its ability to detect multiple objects in an image or video frame with high speed and accuracy. YOLO stands out due to its innovative approach of treating object detection as a single regression problem, enabling it to predict bounding boxes and class probabilities directly from full images in one evaluation.\n",
        "\n",
        "Key Features\n",
        "\n",
        "* Single-Pass Detection: Unlike traditional object detection methods that use a multi-stage process (e.g., region proposal and classification), YOLO processes an image in a single neural network pass, making it extremely fast and suitable for real-time applications.\n",
        "* Grid-Based Prediction: YOLO divides the input image into a grid and assigns each grid cell the responsibility of predicting bounding boxes and their associated class probabilities if the center of an object falls within that cell.\n",
        "* End-to-End Learning: The model is trained end-to-end, optimizing for both object localization and classification simultaneously.\n",
        "* Speed and Efficiency: YOLO is capable of processing images at high frame rates, making it suitable for applications that require real-time performance, such as video surveillance, autonomous vehicles, and interactive systems.\n",
        "\n",
        "## YOLO11\n",
        "[Ultralytics YOLO11](https://docs.ultralytics.com/modes/track/) is a state-of-the-art model that builds on the success of previous YOLO versions, incorporating new features and enhancements to further improve performance and flexibility. **YOLO11** is designed to be fast, accurate, and user-friendly, making it an ideal choice for a variety of tasks, including object detection, tracking, instance segmentation, image classification, and pose estimation.\n",
        "\n",
        "The output from Ultralytics trackers aligns with standard object detection while incorporating object IDs, enabling seamless object tracking in video streams and advanced analytics. Hereâ€™s why Ultralytics YOLO is an excellent choice for your object tracking needs:\n",
        "\n",
        "* **Efficiency**: Processes video streams in real-time with high accuracy.\n",
        "* **Flexibility**: Supports various tracking algorithms and configurable options to suit diverse use cases.\n",
        "* **Ease of Use**: Features a straightforward Python API and CLI for quick setup and deployment.\n",
        "* **Customizability**: Compatible with custom-trained YOLO models, making it ideal for domain-specific applications."
      ],
      "metadata": {
        "id": "qQeIsexnGd-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to `GPU`. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit -> Notebook settings -> Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "REo7t-olJNXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "6BZfkGkQJtom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install YOLO\n",
        "\n",
        "Install the Ultralytics package, along with all required dependencies, in a Python environment (version 3.8 or higher) with `PyTorch` (version 1.8 or higher) using the following command: `pip install ultralytics`."
      ],
      "metadata": {
        "id": "VvGk1sWHJ1m3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo1dZAs6CZe3"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "!mkdir -p {HOME}/data"
      ],
      "metadata": {
        "id": "K8jRc5IECnVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "UQu6LrGNLxVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tahamsi/computer-vision/refs/heads/main/images/people.mp4 -P {HOME}/data\n",
        "source_video = f\"{HOME}/data/people.mp4\"\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "WAEqmZDSCqn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a model\n",
        "\n",
        "You can now choose the core model for your object tracking tasks: either object detection or instance segmentation as the base model. Whichever mode you select, the corresponding checkpoints will be automatically downloaded."
      ],
      "metadata": {
        "id": "hEMAgQBqMoCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolo11n-seg.pt\")"
      ],
      "metadata": {
        "id": "tYmB7v9vDL4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tracking objects in a video using Yolo.track()\n",
        "\n",
        "Object tracking within video analytics stands as a pivotal task, not only discerning object location and classification within frames but also preserving unique identification for each detected object's progression throughout the video. Its applications span a broad spectrum, encompassing areas from surveillance and security to real-time sports analytics.\n",
        "\n",
        "### Available Trackers\n",
        "[Ultralytics YOLO](https://docs.ultralytics.com/modes/track/#real-world-applications) supports the following tracking algorithms, which can be enabled by specifying the corresponding `YAML` configuration file using `tracker=tracker_type.yaml`:\n",
        "\n",
        "* [BoT-SORT](https://github.com/NirAharon/BoT-SORT): Use `botsort.yaml` to enable this tracker.\n",
        "[ByteTrack](https://github.com/ifzhang/ByteTrack): Use `bytetrack.yaml` to enable this tracker.\n",
        "\n",
        "By default, the tracker is set to BoT-SORT.\n"
      ],
      "metadata": {
        "id": "muwUSKO3NgTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(source_video)\n",
        "\n",
        "# Loop through the video frames\n",
        "while cap.isOpened():\n",
        "    # Read a frame from the video\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    if success:\n",
        "\n",
        "        results = model.track(frame, persist=True)\n",
        "\n",
        "        annotated_frame = results[0].plot()\n",
        "\n",
        "        # Display the annotated frame\n",
        "        cv2_imshow(annotated_frame)\n",
        "\n",
        "        # Break the loop if 'q' is pressed\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "    else:\n",
        "        # Break the loop if the end of the video is reached\n",
        "        break\n",
        "\n",
        "# Release the video capture object and close the display window\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "1PiZmuyuCvL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Issues\n",
        "* Presence of undesired classes in the output.\n",
        "* Lack of output in video format.\n"
      ],
      "metadata": {
        "id": "kCtPSCFLY02w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tracking objects in a video using CLI\n",
        "\n",
        "You can directly initiate a Yolov11 model via the CLI mode and download the resulting video from Colab."
      ],
      "metadata": {
        "id": "311eDv1HQNbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd\n",
        "!yolo task = detect mode = predict model = yolo11n-seg.pt conf = 0.25 source = {source_video}"
      ],
      "metadata": {
        "id": "jwo8jJrtC3q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Issues\n",
        "* Presence of undesired classes in the output.\n",
        "* Absence of object tracking functionality."
      ],
      "metadata": {
        "id": "A0_IgftyQTsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('runs/segment/predict/people.avi')"
      ],
      "metadata": {
        "id": "vM1I6DZ6jYqe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}